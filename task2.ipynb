{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAtLh0sm1I7M"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JB4zHBD1OCI",
        "outputId": "6a4f2f0a-acac-4f73-badd-81c923948ab5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVrlJWn71PER",
        "outputId": "9683eec6-fd33-4df8-f2a4-f5df8b5748e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pQfv4bkWeyrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a8dd5c-6cb4-42bc-bf60-8a0a1fd9c6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from utils import AudioProcessing, audioPreprocessing, voting\n",
        "from dataset import audioDataSet\n",
        "from models import *\n",
        "from helper import train_audio, evaluate_audio\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLLJI151I7R"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "OK345_nrpxEU",
        "outputId": "5ce618ec-e38a-4d70-c2aa-c6bf61867698"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-19cddd94-7b50-466d-a6d9-ae906c88f791\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>container id</th>\n",
              "      <th>scenario</th>\n",
              "      <th>background</th>\n",
              "      <th>illumination</th>\n",
              "      <th>width at the top</th>\n",
              "      <th>width at the bottom</th>\n",
              "      <th>height</th>\n",
              "      <th>depth</th>\n",
              "      <th>container capacity</th>\n",
              "      <th>container mass</th>\n",
              "      <th>filling type</th>\n",
              "      <th>filling level</th>\n",
              "      <th>filling density</th>\n",
              "      <th>filling mass</th>\n",
              "      <th>object mass</th>\n",
              "      <th>handover starting frame</th>\n",
              "      <th>handover start timestamp</th>\n",
              "      <th>handover hand</th>\n",
              "      <th>action</th>\n",
              "      <th>nframes</th>\n",
              "      <th>folder_num</th>\n",
              "      <th>file_name</th>\n",
              "      <th>num</th>\n",
              "      <th>subject</th>\n",
              "      <th>filling_type</th>\n",
              "      <th>filling_level</th>\n",
              "      <th>back</th>\n",
              "      <th>light</th>\n",
              "      <th>camera_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82</td>\n",
              "      <td>76.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>291576</td>\n",
              "      <td>2</td>\n",
              "      <td>s2_fi2_fu1_b1_l0</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3209.397</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118483</td>\n",
              "      <td>7</td>\n",
              "      <td>s0_fi0_fu0_b0_l0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>572008</td>\n",
              "      <td>2</td>\n",
              "      <td>s0_fi3_fu1_b1_l0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.40</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1239.840</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141680</td>\n",
              "      <td>8</td>\n",
              "      <td>s0_fi0_fu0_b1_l0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>296.000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34</td>\n",
              "      <td>45.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138681</td>\n",
              "      <td>4</td>\n",
              "      <td>s1_fi1_fu1_b1_l0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19cddd94-7b50-466d-a6d9-ae906c88f791')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19cddd94-7b50-466d-a6d9-ae906c88f791 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19cddd94-7b50-466d-a6d9-ae906c88f791');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id  container id  scenario  background  ...  light  camera_id  start  end\n",
              "0   0             2         2           1  ...      0          2   0.75  3.5\n",
              "1   1             7         0           0  ...      0          2  -1.00 -1.0\n",
              "2   2             2         0           1  ...      0          2   3.40  6.5\n",
              "3   3             8         0           1  ...      0          2  -1.00 -1.0\n",
              "4   4             4         1           1  ...      0          2   0.75  1.8\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gt = pd.read_csv('./files/train.csv')\n",
        "gt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb-WRGUQp1kI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "b45d1890beef4e9eaf00fbf53a676b6a"
          ]
        },
        "outputId": "c6e934d1-70df-4b22-df26-815e1e7ff6d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b45d1890beef4e9eaf00fbf53a676b6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "base_path = '/content/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "mfcc_path = (os.path.join(base_path, 'audios', 'mfcc'))\n",
        "os.makedirs(mfcc_path, exist_ok=True)\n",
        "os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "\n",
        "audioPreprocessing(audio_folder, gt, base_path, mfcc_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESWl3lO21I7T"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTAZbi9Es7KO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e636d3e9ac1a4c7c8b43857197121142",
            "ace71ac4aaa44b05918a69e1ddcfb00d",
            "fa0f261a420a47deb8ded85257414047",
            "2920461c1b4d49099f4c47f809b92b24",
            "cf1a671e92fd4910a6cada9562d6689d",
            "35cb128146ca465b9bdd8959f91214e3",
            "727b5174d04649388c56d3a1b73d1a05",
            "193e779009d64668be7dd9bf0307b5aa",
            "dc8303a70e6c4592830ed02a3ccb48cd",
            "6eca861256a248b1867525eeeeb4a52a",
            "74eb35c35b6445039222f04d6d584f86"
          ]
        },
        "outputId": "38deceb3-4d41-4e5a-cf9b-066bb116ef23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initializing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e636d3e9ac1a4c7c8b43857197121142",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/31812 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mydataset = audioDataSet(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iCXYy9Z__ZhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i67jE5JN1I7U"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oad1iOtNl01a",
        "outputId": "c033f748-1dcd-4f21-93cd-1d1ac609d313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 train loss:1.0360 train acc:73.44% \n",
            "Epoch 1/200 val loss:0.9534 val acc:79.19% \n",
            "Epoch 2/200 train loss:0.9431 train acc:80.36% \n",
            "Epoch 2/200 val loss:0.9258 val acc:81.64% \n",
            "Epoch 3/200 train loss:0.9241 train acc:81.98% \n",
            "Epoch 3/200 val loss:0.9357 val acc:80.56% \n",
            "Epoch 4/200 train loss:0.9132 train acc:82.80% \n",
            "Epoch 4/200 val loss:0.9041 val acc:84.02% \n",
            "Epoch 5/200 train loss:0.8858 train acc:86.28% \n",
            "Epoch 5/200 val loss:1.0089 val acc:72.62% \n",
            "Epoch 6/200 train loss:0.8672 train acc:87.78% \n",
            "Epoch 6/200 val loss:0.8978 val acc:84.50% \n",
            "Epoch 7/200 train loss:0.8605 train acc:88.34% \n",
            "Epoch 7/200 val loss:0.9934 val acc:74.57% \n",
            "Epoch 8/200 train loss:0.8526 train acc:89.10% \n",
            "Epoch 8/200 val loss:0.8889 val acc:85.34% \n",
            "Epoch 9/200 train loss:0.8470 train acc:89.68% \n",
            "Epoch 9/200 val loss:0.8650 val acc:87.79% \n",
            "Epoch 10/200 train loss:0.8426 train acc:90.15% \n",
            "Epoch 10/200 val loss:0.8547 val acc:88.86% \n",
            "Epoch 11/200 train loss:0.8390 train acc:90.55% \n",
            "Epoch 11/200 val loss:0.8496 val acc:89.31% \n",
            "Epoch 12/200 train loss:0.8368 train acc:90.66% \n",
            "Epoch 12/200 val loss:0.8499 val acc:89.23% \n",
            "Epoch 13/200 train loss:0.8327 train acc:91.15% \n",
            "Epoch 13/200 val loss:0.9411 val acc:80.03% \n",
            "Epoch 14/200 train loss:0.8312 train acc:91.28% \n",
            "Epoch 14/200 val loss:0.8509 val acc:89.09% \n",
            "Epoch 15/200 train loss:0.8292 train acc:91.50% \n",
            "Epoch 15/200 val loss:0.9029 val acc:84.03% \n",
            "Epoch 16/200 train loss:0.8248 train acc:91.90% \n",
            "Epoch 16/200 val loss:0.9373 val acc:80.34% \n",
            "Epoch 17/200 train loss:0.8230 train acc:92.13% \n",
            "Epoch 17/200 val loss:0.8491 val acc:89.30% \n",
            "Epoch 18/200 train loss:0.8216 train acc:92.22% \n",
            "Epoch 18/200 val loss:0.8611 val acc:88.28% \n",
            "Epoch 19/200 train loss:0.8209 train acc:92.26% \n",
            "Epoch 19/200 val loss:0.8812 val acc:85.97% \n",
            "Epoch 20/200 train loss:0.8165 train acc:92.81% \n",
            "Epoch 20/200 val loss:0.8674 val acc:87.46% \n",
            "Epoch 21/200 train loss:0.8144 train acc:92.92% \n",
            "Epoch 21/200 val loss:0.8563 val acc:88.59% \n",
            "Epoch 22/200 train loss:0.8153 train acc:92.82% \n",
            "Epoch 22/200 val loss:0.9562 val acc:78.34% \n",
            "Epoch 23/200 train loss:0.8121 train acc:93.14% \n",
            "Epoch 23/200 val loss:0.8666 val acc:87.49% \n",
            "Epoch 24/200 train loss:0.8111 train acc:93.31% \n",
            "Epoch 24/200 val loss:0.8641 val acc:87.87% \n",
            "Epoch 25/200 train loss:0.8098 train acc:93.45% \n",
            "Epoch 25/200 val loss:0.8888 val acc:85.18% \n",
            "Epoch 26/200 train loss:0.8054 train acc:93.87% \n",
            "Epoch 26/200 val loss:0.8687 val acc:87.43% \n",
            "Epoch 27/200 train loss:0.8057 train acc:93.83% \n",
            "Epoch 27/200 val loss:0.8900 val acc:85.01% \n",
            "Epoch 28/200 train loss:0.8062 train acc:93.75% \n",
            "Epoch 28/200 val loss:0.8724 val acc:87.08% \n",
            "Epoch 29/200 train loss:0.8039 train acc:94.01% \n",
            "Epoch 29/200 val loss:0.8435 val acc:89.93% \n",
            "Epoch 30/200 train loss:0.8017 train acc:94.19% \n",
            "Epoch 30/200 val loss:0.9274 val acc:81.25% \n",
            "Epoch 31/200 train loss:0.7998 train acc:94.44% \n",
            "Epoch 31/200 val loss:0.9214 val acc:81.90% \n",
            "Epoch 32/200 train loss:0.8003 train acc:94.32% \n",
            "Epoch 32/200 val loss:0.8517 val acc:89.09% \n",
            "Epoch 33/200 train loss:0.7989 train acc:94.49% \n",
            "Epoch 33/200 val loss:0.8993 val acc:84.28% \n",
            "Epoch 34/200 train loss:0.7974 train acc:94.66% \n",
            "Epoch 34/200 val loss:0.9431 val acc:79.74% \n",
            "Epoch 35/200 train loss:0.7999 train acc:94.38% \n",
            "Epoch 35/200 val loss:0.8753 val acc:86.74% \n",
            "Epoch 36/200 train loss:0.7978 train acc:94.62% \n",
            "Epoch 36/200 val loss:0.8495 val acc:89.34% \n",
            "Epoch 37/200 train loss:0.7952 train acc:94.86% \n",
            "Epoch 37/200 val loss:0.9061 val acc:83.72% \n",
            "Epoch 38/200 train loss:0.7941 train acc:94.96% \n",
            "Epoch 38/200 val loss:0.8587 val acc:88.40% \n",
            "Epoch 39/200 train loss:0.7956 train acc:94.81% \n",
            "Epoch 39/200 val loss:0.8622 val acc:88.06% \n",
            "Epoch 40/200 train loss:0.7923 train acc:95.20% \n",
            "Epoch 40/200 val loss:0.8484 val acc:89.39% \n",
            "Epoch 41/200 train loss:0.7937 train acc:95.02% \n",
            "Epoch 41/200 val loss:0.8468 val acc:89.56% \n",
            "Epoch 42/200 train loss:0.7937 train acc:95.00% \n",
            "Epoch 42/200 val loss:0.8460 val acc:89.66% \n",
            "Epoch 43/200 train loss:0.7906 train acc:95.33% \n",
            "Epoch 43/200 val loss:0.8663 val acc:87.55% \n",
            "Epoch 44/200 train loss:0.7916 train acc:95.21% \n",
            "Epoch 44/200 val loss:0.8472 val acc:89.50% \n",
            "Epoch 45/200 train loss:0.7925 train acc:95.11% \n",
            "Epoch 45/200 val loss:0.9376 val acc:80.42% \n",
            "Epoch 46/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 46/200 val loss:0.8531 val acc:89.01% \n",
            "Epoch 47/200 train loss:0.7907 train acc:95.32% \n",
            "Epoch 47/200 val loss:0.8548 val acc:88.84% \n",
            "Epoch 48/200 train loss:0.7904 train acc:95.33% \n",
            "Epoch 48/200 val loss:0.8680 val acc:87.38% \n",
            "Epoch 49/200 train loss:0.7888 train acc:95.50% \n",
            "Epoch 49/200 val loss:0.8672 val acc:87.57% \n",
            "Epoch 50/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 50/200 val loss:0.8378 val acc:90.46% \n",
            "Epoch 51/200 train loss:0.7879 train acc:95.60% \n",
            "Epoch 51/200 val loss:0.8643 val acc:87.84% \n",
            "Epoch 52/200 train loss:0.7893 train acc:95.45% \n",
            "Epoch 52/200 val loss:0.8572 val acc:88.61% \n",
            "Epoch 53/200 train loss:0.7871 train acc:95.66% \n",
            "Epoch 53/200 val loss:0.8453 val acc:89.67% \n",
            "Epoch 54/200 train loss:0.7874 train acc:95.60% \n",
            "Epoch 54/200 val loss:0.9923 val acc:74.85% \n",
            "Epoch 55/200 train loss:0.7882 train acc:95.53% \n",
            "Epoch 55/200 val loss:0.8462 val acc:89.78% \n",
            "Epoch 56/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 56/200 val loss:0.8948 val acc:84.76% \n",
            "Epoch 57/200 train loss:0.7873 train acc:95.61% \n",
            "Epoch 57/200 val loss:0.8378 val acc:90.48% \n",
            "Epoch 58/200 train loss:0.7843 train acc:95.95% \n",
            "Epoch 58/200 val loss:0.9086 val acc:83.28% \n",
            "Epoch 59/200 train loss:0.7836 train acc:96.05% \n",
            "Epoch 59/200 val loss:0.9462 val acc:79.44% \n",
            "Epoch 60/200 train loss:0.7869 train acc:95.66% \n",
            "Epoch 60/200 val loss:0.8373 val acc:90.46% \n",
            "Epoch 61/200 train loss:0.7848 train acc:95.89% \n",
            "Epoch 61/200 val loss:0.8842 val acc:85.87% \n",
            "Epoch 62/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 62/200 val loss:0.8810 val acc:86.20% \n",
            "Epoch 63/200 train loss:0.7828 train acc:96.10% \n",
            "Epoch 63/200 val loss:0.8509 val acc:89.08% \n",
            "Epoch 64/200 train loss:0.7847 train acc:95.92% \n",
            "Epoch 64/200 val loss:0.8609 val acc:88.29% \n",
            "Epoch 65/200 train loss:0.7868 train acc:95.68% \n",
            "Epoch 65/200 val loss:0.8899 val acc:85.31% \n",
            "Epoch 66/200 train loss:0.7846 train acc:95.92% \n",
            "Epoch 66/200 val loss:0.8507 val acc:89.20% \n",
            "Epoch 67/200 train loss:0.7819 train acc:96.20% \n",
            "Epoch 67/200 val loss:0.8515 val acc:89.11% \n",
            "Epoch 68/200 train loss:0.7820 train acc:96.17% \n",
            "Epoch 68/200 val loss:0.8586 val acc:88.34% \n",
            "Epoch 69/200 train loss:0.7822 train acc:96.18% \n",
            "Epoch 69/200 val loss:0.8477 val acc:89.53% \n",
            "Epoch 70/200 train loss:0.7822 train acc:96.15% \n",
            "Epoch 70/200 val loss:0.8470 val acc:89.61% \n",
            "Epoch 71/200 train loss:0.7816 train acc:96.22% \n",
            "Epoch 71/200 val loss:0.8529 val acc:89.01% \n",
            "Epoch 72/200 train loss:0.7817 train acc:96.20% \n",
            "Epoch 72/200 val loss:0.8500 val acc:89.30% \n",
            "Epoch 73/200 train loss:0.7813 train acc:96.24% \n",
            "Epoch 73/200 val loss:0.8852 val acc:85.64% \n",
            "Epoch 74/200 train loss:0.7810 train acc:96.28% \n",
            "Epoch 74/200 val loss:0.9094 val acc:83.23% \n",
            "Epoch 75/200 train loss:0.7812 train acc:96.24% \n",
            "Epoch 75/200 val loss:0.8414 val acc:90.08% \n",
            "Epoch 76/200 train loss:0.7792 train acc:96.45% \n",
            "Epoch 76/200 val loss:0.8361 val acc:90.66% \n",
            "Epoch 77/200 train loss:0.7795 train acc:96.41% \n",
            "Epoch 77/200 val loss:0.8692 val acc:87.40% \n",
            "Epoch 78/200 train loss:0.7811 train acc:96.25% \n",
            "Epoch 78/200 val loss:0.8439 val acc:89.85% \n",
            "Epoch 79/200 train loss:0.7792 train acc:96.44% \n",
            "Epoch 79/200 val loss:0.8452 val acc:89.78% \n",
            "Epoch 80/200 train loss:0.7810 train acc:96.25% \n",
            "Epoch 80/200 val loss:0.8821 val acc:85.95% \n",
            "Epoch 81/200 train loss:0.7811 train acc:96.27% \n",
            "Epoch 81/200 val loss:0.8502 val acc:89.27% \n",
            "Epoch 82/200 train loss:0.7788 train acc:96.49% \n",
            "Epoch 82/200 val loss:0.8486 val acc:89.31% \n",
            "Epoch 83/200 train loss:0.7796 train acc:96.40% \n",
            "Epoch 83/200 val loss:0.8372 val acc:90.52% \n",
            "Epoch 84/200 train loss:0.7778 train acc:96.60% \n",
            "Epoch 84/200 val loss:0.8523 val acc:89.01% \n",
            "Epoch 85/200 train loss:0.7783 train acc:96.52% \n",
            "Epoch 85/200 val loss:0.8463 val acc:89.58% \n",
            "Epoch 86/200 train loss:0.7795 train acc:96.42% \n",
            "Epoch 86/200 val loss:0.8651 val acc:87.66% \n",
            "Epoch 87/200 train loss:0.7785 train acc:96.50% \n",
            "Epoch 87/200 val loss:0.8349 val acc:90.77% \n",
            "Epoch 88/200 train loss:0.7772 train acc:96.65% \n",
            "Epoch 88/200 val loss:0.8455 val acc:89.75% \n",
            "Epoch 89/200 train loss:0.7784 train acc:96.52% \n",
            "Epoch 89/200 val loss:0.8466 val acc:89.66% \n",
            "Epoch 90/200 train loss:0.7769 train acc:96.67% \n",
            "Epoch 90/200 val loss:0.8400 val acc:90.33% \n",
            "Epoch 91/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 91/200 val loss:0.8592 val acc:88.20% \n",
            "Epoch 92/200 train loss:0.7776 train acc:96.57% \n",
            "Epoch 92/200 val loss:0.8464 val acc:89.60% \n",
            "Epoch 93/200 train loss:0.7758 train acc:96.81% \n",
            "Epoch 93/200 val loss:0.8465 val acc:89.60% \n",
            "Epoch 94/200 train loss:0.7771 train acc:96.64% \n",
            "Epoch 94/200 val loss:0.8441 val acc:89.86% \n",
            "Epoch 95/200 train loss:0.7760 train acc:96.74% \n",
            "Epoch 95/200 val loss:0.9288 val acc:81.08% \n",
            "Epoch 96/200 train loss:0.7787 train acc:96.51% \n",
            "Epoch 96/200 val loss:0.8500 val acc:89.31% \n",
            "Epoch 97/200 train loss:0.7767 train acc:96.70% \n",
            "Epoch 97/200 val loss:0.8453 val acc:89.85% \n",
            "Epoch 98/200 train loss:0.7780 train acc:96.54% \n",
            "Epoch 98/200 val loss:1.0982 val acc:64.04% \n",
            "Epoch 99/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 99/200 val loss:0.8364 val acc:90.57% \n",
            "Epoch 100/200 train loss:0.7761 train acc:96.78% \n",
            "Epoch 100/200 val loss:0.8379 val acc:90.49% \n",
            "Epoch 101/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 101/200 val loss:0.8405 val acc:90.29% \n",
            "Epoch 102/200 train loss:0.7745 train acc:96.93% \n",
            "Epoch 102/200 val loss:0.8370 val acc:90.63% \n",
            "Epoch 103/200 train loss:0.7753 train acc:96.81% \n",
            "Epoch 103/200 val loss:0.8485 val acc:89.49% \n",
            "Epoch 104/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 104/200 val loss:0.8416 val acc:90.11% \n",
            "Epoch 105/200 train loss:0.7737 train acc:97.02% \n",
            "Epoch 105/200 val loss:0.8423 val acc:90.11% \n",
            "Epoch 106/200 train loss:0.7752 train acc:96.85% \n",
            "Epoch 106/200 val loss:0.8812 val acc:86.08% \n",
            "Epoch 107/200 train loss:0.7770 train acc:96.68% \n",
            "Epoch 107/200 val loss:0.8585 val acc:88.42% \n",
            "Epoch 108/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 108/200 val loss:0.8552 val acc:88.68% \n",
            "Epoch 109/200 train loss:0.7760 train acc:96.75% \n",
            "Epoch 109/200 val loss:0.8396 val acc:90.26% \n",
            "Epoch 110/200 train loss:0.7746 train acc:96.90% \n",
            "Epoch 110/200 val loss:0.8346 val acc:90.82% \n",
            "Epoch 111/200 train loss:0.7726 train acc:97.10% \n",
            "Epoch 111/200 val loss:0.8397 val acc:90.30% \n",
            "Epoch 112/200 train loss:0.7753 train acc:96.83% \n",
            "Epoch 112/200 val loss:0.8417 val acc:90.05% \n",
            "Epoch 113/200 train loss:0.7725 train acc:97.14% \n",
            "Epoch 113/200 val loss:0.8344 val acc:90.93% \n",
            "Epoch 114/200 train loss:0.7720 train acc:97.18% \n",
            "Epoch 114/200 val loss:0.8601 val acc:88.23% \n",
            "Epoch 115/200 train loss:0.7728 train acc:97.08% \n",
            "Epoch 115/200 val loss:0.8377 val acc:90.52% \n",
            "Epoch 116/200 train loss:0.7731 train acc:97.05% \n",
            "Epoch 116/200 val loss:0.8358 val acc:90.62% \n",
            "Epoch 117/200 train loss:0.7729 train acc:97.08% \n",
            "Epoch 117/200 val loss:0.8495 val acc:89.36% \n",
            "Epoch 118/200 train loss:0.7749 train acc:96.86% \n",
            "Epoch 118/200 val loss:0.8459 val acc:89.74% \n",
            "Epoch 119/200 train loss:0.7740 train acc:96.96% \n",
            "Epoch 119/200 val loss:0.8355 val acc:90.70% \n",
            "Epoch 120/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 120/200 val loss:0.8610 val acc:88.04% \n",
            "Epoch 121/200 train loss:0.7738 train acc:97.01% \n",
            "Epoch 121/200 val loss:0.8367 val acc:90.66% \n",
            "Epoch 122/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 122/200 val loss:0.8313 val acc:91.15% \n",
            "Epoch 123/200 train loss:0.7713 train acc:97.23% \n",
            "Epoch 123/200 val loss:0.8518 val acc:88.92% \n",
            "Epoch 124/200 train loss:0.7714 train acc:97.23% \n",
            "Epoch 124/200 val loss:0.8609 val acc:88.20% \n",
            "Epoch 125/200 train loss:0.7718 train acc:97.21% \n",
            "Epoch 125/200 val loss:0.8441 val acc:89.96% \n",
            "Epoch 126/200 train loss:0.7703 train acc:97.35% \n",
            "Epoch 126/200 val loss:0.8379 val acc:90.44% \n",
            "Epoch 127/200 train loss:0.7707 train acc:97.32% \n",
            "Epoch 127/200 val loss:0.8412 val acc:90.16% \n",
            "Epoch 128/200 train loss:0.7718 train acc:97.20% \n",
            "Epoch 128/200 val loss:0.8416 val acc:90.05% \n",
            "Epoch 129/200 train loss:0.7737 train acc:96.98% \n",
            "Epoch 129/200 val loss:0.8949 val acc:84.71% \n",
            "Epoch 130/200 train loss:0.7725 train acc:97.12% \n",
            "Epoch 130/200 val loss:0.9009 val acc:84.14% \n",
            "Epoch 131/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 131/200 val loss:0.8399 val acc:90.41% \n",
            "Epoch 132/200 train loss:0.7704 train acc:97.33% \n",
            "Epoch 132/200 val loss:0.8673 val acc:87.52% \n",
            "Epoch 133/200 train loss:0.7725 train acc:97.10% \n",
            "Epoch 133/200 val loss:0.9922 val acc:74.71% \n",
            "Epoch 134/200 train loss:0.7711 train acc:97.26% \n",
            "Epoch 134/200 val loss:0.8365 val acc:90.65% \n",
            "Epoch 135/200 train loss:0.7698 train acc:97.39% \n",
            "Epoch 135/200 val loss:0.8395 val acc:90.32% \n",
            "Epoch 136/200 train loss:0.7706 train acc:97.32% \n",
            "Epoch 136/200 val loss:0.8370 val acc:90.55% \n",
            "Epoch 137/200 train loss:0.7712 train acc:97.25% \n",
            "Epoch 137/200 val loss:0.9025 val acc:83.89% \n",
            "Epoch 138/200 train loss:0.7697 train acc:97.41% \n",
            "Epoch 138/200 val loss:0.8335 val acc:90.90% \n",
            "Epoch 139/200 train loss:0.7695 train acc:97.38% \n",
            "Epoch 139/200 val loss:0.8545 val acc:88.84% \n",
            "Epoch 140/200 train loss:0.7700 train acc:97.36% \n",
            "Epoch 140/200 val loss:0.8449 val acc:89.75% \n",
            "Epoch 141/200 train loss:0.7696 train acc:97.40% \n",
            "Epoch 141/200 val loss:0.8430 val acc:89.93% \n",
            "Epoch 142/200 train loss:0.7684 train acc:97.52% \n",
            "Epoch 142/200 val loss:0.8360 val acc:90.65% \n",
            "Epoch 143/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 143/200 val loss:1.0154 val acc:72.54% \n",
            "Epoch 144/200 train loss:0.7706 train acc:97.30% \n",
            "Epoch 144/200 val loss:0.8374 val acc:90.54% \n",
            "Epoch 145/200 train loss:0.7705 train acc:97.31% \n",
            "Epoch 145/200 val loss:0.8809 val acc:85.98% \n",
            "Epoch 146/200 train loss:0.7700 train acc:97.38% \n",
            "Epoch 146/200 val loss:0.8518 val acc:89.08% \n",
            "Epoch 147/200 train loss:0.7714 train acc:97.21% \n",
            "Epoch 147/200 val loss:0.8347 val acc:90.84% \n",
            "Epoch 148/200 train loss:0.7699 train acc:97.38% \n",
            "Epoch 148/200 val loss:0.8337 val acc:90.92% \n",
            "Epoch 149/200 train loss:0.7694 train acc:97.42% \n",
            "Epoch 149/200 val loss:0.8342 val acc:90.87% \n",
            "Epoch 150/200 train loss:0.7696 train acc:97.42% \n",
            "Epoch 150/200 val loss:0.8486 val acc:89.38% \n",
            "Epoch 151/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 151/200 val loss:0.8508 val acc:89.22% \n",
            "Epoch 152/200 train loss:0.7672 train acc:97.67% \n",
            "Epoch 152/200 val loss:0.8418 val acc:90.07% \n",
            "Epoch 153/200 train loss:0.7715 train acc:97.22% \n",
            "Epoch 153/200 val loss:0.8484 val acc:89.42% \n",
            "Epoch 154/200 train loss:0.7695 train acc:97.42% \n",
            "Epoch 154/200 val loss:0.8357 val acc:90.66% \n",
            "Epoch 155/200 train loss:0.7686 train acc:97.51% \n",
            "Epoch 155/200 val loss:0.8302 val acc:91.37% \n",
            "Epoch 156/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 156/200 val loss:0.8541 val acc:88.75% \n",
            "Epoch 157/200 train loss:0.7687 train acc:97.50% \n",
            "Epoch 157/200 val loss:0.8269 val acc:91.62% \n",
            "Epoch 158/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 158/200 val loss:0.8364 val acc:90.59% \n",
            "Epoch 159/200 train loss:0.7683 train acc:97.52% \n",
            "Epoch 159/200 val loss:0.8285 val acc:91.39% \n",
            "Epoch 160/200 train loss:0.7706 train acc:97.29% \n",
            "Epoch 160/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 161/200 train loss:0.7687 train acc:97.49% \n",
            "Epoch 161/200 val loss:0.8408 val acc:90.21% \n",
            "Epoch 162/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 162/200 val loss:0.8290 val acc:91.43% \n",
            "Epoch 163/200 train loss:0.7675 train acc:97.62% \n",
            "Epoch 163/200 val loss:0.8245 val acc:91.87% \n",
            "Epoch 164/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 164/200 val loss:0.8353 val acc:90.79% \n",
            "Epoch 165/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 165/200 val loss:0.8392 val acc:90.33% \n",
            "Epoch 166/200 train loss:0.7677 train acc:97.58% \n",
            "Epoch 166/200 val loss:0.8336 val acc:90.92% \n",
            "Epoch 167/200 train loss:0.7689 train acc:97.47% \n",
            "Epoch 167/200 val loss:0.8816 val acc:86.19% \n",
            "Epoch 168/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 168/200 val loss:0.8264 val acc:91.64% \n",
            "Epoch 169/200 train loss:0.7677 train acc:97.61% \n",
            "Epoch 169/200 val loss:0.8368 val acc:90.62% \n",
            "Epoch 170/200 train loss:0.7669 train acc:97.68% \n",
            "Epoch 170/200 val loss:0.9737 val acc:76.83% \n",
            "Epoch 171/200 train loss:0.7694 train acc:97.41% \n",
            "Epoch 171/200 val loss:0.8413 val acc:90.16% \n",
            "Epoch 172/200 train loss:0.7667 train acc:97.68% \n",
            "Epoch 172/200 val loss:0.8304 val acc:91.26% \n",
            "Epoch 173/200 train loss:0.7668 train acc:97.70% \n",
            "Epoch 173/200 val loss:0.8572 val acc:88.43% \n",
            "Epoch 174/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 174/200 val loss:0.8916 val acc:85.07% \n",
            "Epoch 175/200 train loss:0.7669 train acc:97.69% \n",
            "Epoch 175/200 val loss:0.8388 val acc:90.33% \n",
            "Epoch 176/200 train loss:0.7656 train acc:97.82% \n",
            "Epoch 176/200 val loss:0.8344 val acc:90.84% \n",
            "Epoch 177/200 train loss:0.7657 train acc:97.80% \n",
            "Epoch 177/200 val loss:0.8368 val acc:90.55% \n",
            "Epoch 178/200 train loss:0.7695 train acc:97.43% \n",
            "Epoch 178/200 val loss:0.8321 val acc:91.03% \n",
            "Epoch 179/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 179/200 val loss:0.8282 val acc:91.47% \n",
            "Epoch 180/200 train loss:0.7667 train acc:97.72% \n",
            "Epoch 180/200 val loss:0.8643 val acc:87.71% \n",
            "Epoch 181/200 train loss:0.7673 train acc:97.63% \n",
            "Epoch 181/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 182/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 182/200 val loss:0.8569 val acc:88.50% \n",
            "Epoch 183/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 183/200 val loss:0.9029 val acc:83.81% \n",
            "Epoch 184/200 train loss:0.7677 train acc:97.60% \n",
            "Epoch 184/200 val loss:0.8271 val acc:91.67% \n",
            "Epoch 185/200 train loss:0.7650 train acc:97.87% \n",
            "Epoch 185/200 val loss:0.8253 val acc:91.75% \n",
            "Epoch 186/200 train loss:0.7651 train acc:97.85% \n",
            "Epoch 186/200 val loss:0.8378 val acc:90.52% \n",
            "Epoch 187/200 train loss:0.7656 train acc:97.79% \n",
            "Epoch 187/200 val loss:0.8965 val acc:84.54% \n",
            "Epoch 188/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 188/200 val loss:0.8632 val acc:87.99% \n",
            "Epoch 189/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 189/200 val loss:0.8347 val acc:90.74% \n",
            "Epoch 190/200 train loss:0.7665 train acc:97.71% \n",
            "Epoch 190/200 val loss:0.8268 val acc:91.54% \n",
            "Epoch 191/200 train loss:0.7657 train acc:97.79% \n",
            "Epoch 191/200 val loss:0.8259 val acc:91.73% \n",
            "Epoch 192/200 train loss:0.7652 train acc:97.84% \n",
            "Epoch 192/200 val loss:0.8213 val acc:92.19% \n",
            "Epoch 193/200 train loss:0.7676 train acc:97.60% \n",
            "Epoch 193/200 val loss:0.8603 val acc:88.23% \n",
            "Epoch 194/200 train loss:0.7690 train acc:97.46% \n",
            "Epoch 194/200 val loss:0.8516 val acc:88.97% \n",
            "Epoch 195/200 train loss:0.7667 train acc:97.70% \n",
            "Epoch 195/200 val loss:0.8308 val acc:91.10% \n",
            "Epoch 196/200 train loss:0.7648 train acc:97.89% \n",
            "Epoch 196/200 val loss:0.8268 val acc:91.62% \n",
            "Epoch 197/200 train loss:0.7658 train acc:97.77% \n",
            "Epoch 197/200 val loss:0.8396 val acc:90.51% \n",
            "Epoch 198/200 train loss:0.7652 train acc:97.83% \n",
            "Epoch 198/200 val loss:0.8366 val acc:90.62% \n",
            "Epoch 199/200 train loss:0.7671 train acc:97.65% \n",
            "Epoch 199/200 val loss:0.8394 val acc:90.40% \n",
            "Epoch 200/200 train loss:0.7658 train acc:97.78% \n",
            "Epoch 200/200 val loss:0.8389 val acc:90.37% \n"
          ]
        }
      ],
      "source": [
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-5\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_val.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho3pWlGf1I7W"
      },
      "source": [
        "## MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPonh4SL1_M9",
        "outputId": "8556c6bd-9364-4e66-a6fe-7eb3112f461b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:0.6436 train acc:76.81% val loss:0.6356 val acc:75.44%\n",
            "2/200 train loss:0.3759 train acc:87.12% val loss:0.8538 val acc:69.65%\n",
            "3/200 train loss:0.3205 train acc:89.04% val loss:0.3209 val acc:88.75%\n",
            "4/200 train loss:0.2887 train acc:89.76% val loss:0.3830 val acc:88.10%\n",
            "5/200 train loss:0.2623 train acc:90.82% val loss:4.0947 val acc:37.92%\n",
            "6/200 train loss:0.2388 train acc:91.45% val loss:0.5306 val acc:85.60%\n",
            "7/200 train loss:0.2206 train acc:92.23% val loss:0.4271 val acc:85.93%\n",
            "8/200 train loss:0.2042 train acc:92.79% val loss:0.3086 val acc:89.66%\n",
            "9/200 train loss:0.1900 train acc:93.00% val loss:0.3779 val acc:87.02%\n",
            "10/200 train loss:0.1671 train acc:93.95% val loss:0.4665 val acc:86.23%\n",
            "11/200 train loss:0.1490 train acc:94.59% val loss:0.4374 val acc:84.21%\n",
            "12/200 train loss:0.1354 train acc:95.16% val loss:0.4699 val acc:84.11%\n",
            "13/200 train loss:0.1264 train acc:95.49% val loss:0.5517 val acc:86.75%\n",
            "14/200 train loss:0.1105 train acc:95.83% val loss:0.5975 val acc:82.51%\n",
            "15/200 train loss:0.0909 train acc:96.73% val loss:0.5483 val acc:87.46%\n",
            "16/200 train loss:0.0849 train acc:96.82% val loss:0.6471 val acc:80.37%\n",
            "17/200 train loss:0.0747 train acc:97.38% val loss:0.4912 val acc:89.89%\n",
            "18/200 train loss:0.0755 train acc:97.21% val loss:0.3970 val acc:87.93%\n",
            "19/200 train loss:0.0654 train acc:97.78% val loss:0.3595 val acc:89.53%\n",
            "20/200 train loss:0.0526 train acc:98.09% val loss:0.3757 val acc:91.20%\n",
            "21/200 train loss:0.0535 train acc:98.06% val loss:0.4843 val acc:90.71%\n",
            "22/200 train loss:0.0484 train acc:98.24% val loss:0.3753 val acc:90.88%\n",
            "23/200 train loss:0.0534 train acc:98.19% val loss:0.3942 val acc:89.72%\n",
            "24/200 train loss:0.0446 train acc:98.44% val loss:0.5791 val acc:88.84%\n",
            "25/200 train loss:0.0479 train acc:98.26% val loss:0.3874 val acc:91.45%\n",
            "26/200 train loss:0.0411 train acc:98.60% val loss:0.3357 val acc:92.25%\n",
            "27/200 train loss:0.0499 train acc:98.26% val loss:0.3548 val acc:90.68%\n",
            "28/200 train loss:0.0347 train acc:98.72% val loss:0.4968 val acc:88.81%\n",
            "29/200 train loss:0.0397 train acc:98.68% val loss:0.6247 val acc:81.24%\n",
            "30/200 train loss:0.0336 train acc:98.86% val loss:0.5043 val acc:87.88%\n",
            "31/200 train loss:0.0389 train acc:98.69% val loss:0.4023 val acc:91.56%\n",
            "32/200 train loss:0.0313 train acc:98.92% val loss:0.3813 val acc:91.84%\n",
            "33/200 train loss:0.0320 train acc:98.86% val loss:0.4522 val acc:89.49%\n",
            "34/200 train loss:0.0320 train acc:98.90% val loss:0.6965 val acc:83.51%\n",
            "35/200 train loss:0.0345 train acc:98.84% val loss:0.3720 val acc:91.12%\n",
            "36/200 train loss:0.0281 train acc:99.03% val loss:0.5478 val acc:86.30%\n",
            "37/200 train loss:0.0299 train acc:98.90% val loss:0.2545 val acc:93.98%\n",
            "38/200 train loss:0.0306 train acc:98.99% val loss:0.3202 val acc:91.07%\n",
            "39/200 train loss:0.0276 train acc:99.00% val loss:0.7015 val acc:86.11%\n",
            "40/200 train loss:0.0339 train acc:98.79% val loss:0.5578 val acc:88.26%\n",
            "41/200 train loss:0.0270 train acc:99.06% val loss:0.4575 val acc:91.48%\n",
            "42/200 train loss:0.0242 train acc:99.16% val loss:0.7295 val acc:87.43%\n",
            "43/200 train loss:0.0304 train acc:98.98% val loss:0.7459 val acc:86.59%\n",
            "44/200 train loss:0.0315 train acc:98.94% val loss:0.5310 val acc:91.12%\n",
            "45/200 train loss:0.0240 train acc:99.17% val loss:0.3719 val acc:90.55%\n",
            "46/200 train loss:0.0227 train acc:99.26% val loss:0.5446 val acc:91.17%\n",
            "47/200 train loss:0.0254 train acc:99.12% val loss:0.4377 val acc:90.65%\n",
            "48/200 train loss:0.0240 train acc:99.25% val loss:1.2009 val acc:72.36%\n",
            "49/200 train loss:0.0342 train acc:98.81% val loss:0.2588 val acc:93.10%\n",
            "50/200 train loss:0.0168 train acc:99.42% val loss:0.3432 val acc:93.53%\n",
            "51/200 train loss:0.0233 train acc:99.23% val loss:0.2730 val acc:93.64%\n",
            "52/200 train loss:0.0242 train acc:99.23% val loss:0.3682 val acc:92.00%\n",
            "53/200 train loss:0.0204 train acc:99.35% val loss:0.4094 val acc:89.31%\n",
            "54/200 train loss:0.0196 train acc:99.34% val loss:0.4719 val acc:89.03%\n",
            "55/200 train loss:0.0249 train acc:99.11% val loss:0.4283 val acc:91.42%\n",
            "56/200 train loss:0.0219 train acc:99.23% val loss:0.7742 val acc:86.88%\n",
            "57/200 train loss:0.0183 train acc:99.45% val loss:0.6011 val acc:88.04%\n",
            "58/200 train loss:0.0218 train acc:99.30% val loss:0.4043 val acc:90.46%\n",
            "59/200 train loss:0.0163 train acc:99.50% val loss:0.7328 val acc:88.97%\n",
            "60/200 train loss:0.0221 train acc:99.15% val loss:0.5047 val acc:87.47%\n",
            "61/200 train loss:0.0207 train acc:99.26% val loss:0.5624 val acc:90.73%\n",
            "62/200 train loss:0.0206 train acc:99.31% val loss:0.3368 val acc:93.34%\n",
            "63/200 train loss:0.0159 train acc:99.49% val loss:0.3246 val acc:92.90%\n",
            "64/200 train loss:0.0226 train acc:99.22% val loss:0.2303 val acc:95.07%\n",
            "65/200 train loss:0.0150 train acc:99.50% val loss:0.3238 val acc:92.27%\n",
            "66/200 train loss:0.0196 train acc:99.31% val loss:0.4491 val acc:91.65%\n",
            "67/200 train loss:0.0202 train acc:99.30% val loss:0.3068 val acc:93.71%\n",
            "68/200 train loss:0.0125 train acc:99.61% val loss:1.0067 val acc:85.42%\n",
            "69/200 train loss:0.0218 train acc:99.29% val loss:0.9728 val acc:86.45%\n",
            "70/200 train loss:0.0262 train acc:99.14% val loss:0.2335 val acc:93.62%\n",
            "71/200 train loss:0.0280 train acc:99.03% val loss:0.3125 val acc:93.04%\n",
            "72/200 train loss:0.0086 train acc:99.72% val loss:0.5024 val acc:91.58%\n",
            "73/200 train loss:0.0112 train acc:99.64% val loss:0.3637 val acc:92.74%\n",
            "74/200 train loss:0.0149 train acc:99.49% val loss:0.6031 val acc:85.92%\n",
            "75/200 train loss:0.0135 train acc:99.59% val loss:0.5208 val acc:91.25%\n",
            "76/200 train loss:0.0135 train acc:99.58% val loss:0.4604 val acc:92.14%\n",
            "77/200 train loss:0.0199 train acc:99.31% val loss:0.5889 val acc:88.48%\n",
            "78/200 train loss:0.0149 train acc:99.50% val loss:0.3969 val acc:92.11%\n",
            "79/200 train loss:0.0181 train acc:99.41% val loss:0.7074 val acc:89.61%\n",
            "80/200 train loss:0.0161 train acc:99.46% val loss:0.4768 val acc:89.33%\n",
            "81/200 train loss:0.0154 train acc:99.51% val loss:0.4102 val acc:92.27%\n",
            "82/200 train loss:0.0170 train acc:99.41% val loss:0.4261 val acc:91.56%\n",
            "83/200 train loss:0.0100 train acc:99.70% val loss:0.2823 val acc:93.48%\n",
            "84/200 train loss:0.0177 train acc:99.48% val loss:0.3092 val acc:93.48%\n",
            "85/200 train loss:0.0136 train acc:99.56% val loss:0.2574 val acc:94.55%\n",
            "86/200 train loss:0.0155 train acc:99.47% val loss:0.5405 val acc:88.54%\n",
            "87/200 train loss:0.0159 train acc:99.52% val loss:0.3096 val acc:94.17%\n",
            "88/200 train loss:0.0141 train acc:99.58% val loss:0.3581 val acc:92.77%\n",
            "89/200 train loss:0.0119 train acc:99.61% val loss:0.4340 val acc:92.14%\n",
            "90/200 train loss:0.0124 train acc:99.60% val loss:0.2383 val acc:94.81%\n",
            "91/200 train loss:0.0177 train acc:99.43% val loss:0.3375 val acc:93.15%\n",
            "92/200 train loss:0.0112 train acc:99.64% val loss:0.3156 val acc:93.15%\n",
            "93/200 train loss:0.0083 train acc:99.74% val loss:0.3052 val acc:93.76%\n",
            "94/200 train loss:0.0149 train acc:99.49% val loss:0.4189 val acc:92.77%\n",
            "95/200 train loss:0.0161 train acc:99.49% val loss:0.2335 val acc:94.58%\n",
            "96/200 train loss:0.0110 train acc:99.62% val loss:0.3082 val acc:93.38%\n",
            "97/200 train loss:0.0205 train acc:99.33% val loss:0.6259 val acc:86.66%\n",
            "98/200 train loss:0.0080 train acc:99.74% val loss:0.3074 val acc:93.27%\n",
            "99/200 train loss:0.0057 train acc:99.87% val loss:0.2706 val acc:94.77%\n",
            "100/200 train loss:0.0163 train acc:99.47% val loss:0.3105 val acc:94.20%\n",
            "101/200 train loss:0.0126 train acc:99.56% val loss:0.7588 val acc:86.85%\n",
            "102/200 train loss:0.0131 train acc:99.56% val loss:0.2877 val acc:94.63%\n",
            "103/200 train loss:0.0098 train acc:99.67% val loss:0.2653 val acc:93.79%\n",
            "104/200 train loss:0.0140 train acc:99.56% val loss:0.2972 val acc:94.26%\n",
            "105/200 train loss:0.0093 train acc:99.69% val loss:0.2436 val acc:94.69%\n",
            "106/200 train loss:0.0159 train acc:99.48% val loss:0.4434 val acc:90.21%\n",
            "107/200 train loss:0.0105 train acc:99.65% val loss:0.5098 val acc:90.30%\n",
            "108/200 train loss:0.0116 train acc:99.63% val loss:0.3040 val acc:93.68%\n",
            "109/200 train loss:0.0116 train acc:99.62% val loss:0.4879 val acc:91.56%\n",
            "110/200 train loss:0.0202 train acc:99.43% val loss:0.5717 val acc:85.43%\n",
            "111/200 train loss:0.0175 train acc:99.38% val loss:0.3773 val acc:91.62%\n",
            "112/200 train loss:0.0151 train acc:99.54% val loss:0.3586 val acc:92.57%\n",
            "113/200 train loss:0.0092 train acc:99.73% val loss:0.4873 val acc:91.34%\n",
            "114/200 train loss:0.0094 train acc:99.68% val loss:0.2845 val acc:94.55%\n",
            "115/200 train loss:0.0101 train acc:99.66% val loss:0.4853 val acc:90.49%\n",
            "116/200 train loss:0.0132 train acc:99.56% val loss:0.2732 val acc:94.34%\n",
            "117/200 train loss:0.0096 train acc:99.69% val loss:0.3180 val acc:94.12%\n",
            "118/200 train loss:0.0101 train acc:99.71% val loss:0.6035 val acc:88.26%\n",
            "119/200 train loss:0.0115 train acc:99.63% val loss:0.6277 val acc:90.07%\n",
            "120/200 train loss:0.0120 train acc:99.61% val loss:0.3440 val acc:93.53%\n",
            "121/200 train loss:0.0074 train acc:99.73% val loss:0.3864 val acc:91.40%\n",
            "122/200 train loss:0.0146 train acc:99.56% val loss:0.4472 val acc:89.58%\n",
            "123/200 train loss:0.0068 train acc:99.79% val loss:0.4148 val acc:91.91%\n",
            "124/200 train loss:0.0085 train acc:99.70% val loss:0.4612 val acc:92.39%\n",
            "125/200 train loss:0.0144 train acc:99.50% val loss:0.6780 val acc:87.58%\n",
            "126/200 train loss:0.0089 train acc:99.71% val loss:0.3697 val acc:93.29%\n",
            "127/200 train loss:0.0073 train acc:99.77% val loss:0.5685 val acc:91.06%\n",
            "128/200 train loss:0.0124 train acc:99.57% val loss:0.4070 val acc:93.29%\n",
            "129/200 train loss:0.0116 train acc:99.63% val loss:0.5907 val acc:91.50%\n",
            "130/200 train loss:0.0117 train acc:99.61% val loss:0.3267 val acc:94.00%\n",
            "131/200 train loss:0.0128 train acc:99.58% val loss:0.3125 val acc:92.85%\n",
            "132/200 train loss:0.0086 train acc:99.71% val loss:0.7650 val acc:85.15%\n",
            "133/200 train loss:0.0078 train acc:99.77% val loss:0.5065 val acc:91.21%\n",
            "134/200 train loss:0.0133 train acc:99.58% val loss:0.4963 val acc:89.28%\n",
            "135/200 train loss:0.0140 train acc:99.48% val loss:0.2564 val acc:94.33%\n",
            "136/200 train loss:0.0083 train acc:99.75% val loss:0.3872 val acc:92.49%\n",
            "137/200 train loss:0.0061 train acc:99.77% val loss:0.2595 val acc:95.05%\n",
            "138/200 train loss:0.0057 train acc:99.83% val loss:0.2696 val acc:94.83%\n",
            "139/200 train loss:0.0189 train acc:99.39% val loss:0.3446 val acc:93.35%\n",
            "140/200 train loss:0.0096 train acc:99.69% val loss:0.2472 val acc:94.91%\n",
            "141/200 train loss:0.0075 train acc:99.75% val loss:0.2807 val acc:95.22%\n",
            "142/200 train loss:0.0055 train acc:99.83% val loss:0.4893 val acc:91.34%\n",
            "143/200 train loss:0.0066 train acc:99.82% val loss:0.4454 val acc:93.12%\n",
            "144/200 train loss:0.0119 train acc:99.63% val loss:0.4485 val acc:91.12%\n",
            "145/200 train loss:0.0080 train acc:99.75% val loss:0.2332 val acc:95.14%\n",
            "146/200 train loss:0.0081 train acc:99.72% val loss:0.3491 val acc:93.57%\n",
            "147/200 train loss:0.0148 train acc:99.54% val loss:0.3254 val acc:92.57%\n",
            "148/200 train loss:0.0084 train acc:99.74% val loss:0.5324 val acc:91.51%\n",
            "149/200 train loss:0.0052 train acc:99.79% val loss:0.2577 val acc:95.44%\n",
            "150/200 train loss:0.0088 train acc:99.69% val loss:0.3094 val acc:93.48%\n",
            "151/200 train loss:0.0115 train acc:99.62% val loss:0.4328 val acc:93.04%\n",
            "152/200 train loss:0.0075 train acc:99.74% val loss:0.2444 val acc:95.38%\n",
            "153/200 train loss:0.0066 train acc:99.78% val loss:0.3923 val acc:93.09%\n",
            "154/200 train loss:0.0133 train acc:99.58% val loss:0.4249 val acc:92.82%\n",
            "155/200 train loss:0.0091 train acc:99.72% val loss:0.2435 val acc:94.39%\n",
            "156/200 train loss:0.0056 train acc:99.82% val loss:0.2518 val acc:95.18%\n",
            "157/200 train loss:0.0084 train acc:99.71% val loss:0.2552 val acc:95.03%\n",
            "158/200 train loss:0.0067 train acc:99.78% val loss:0.3003 val acc:94.33%\n",
            "159/200 train loss:0.0091 train acc:99.69% val loss:0.9435 val acc:84.00%\n",
            "160/200 train loss:0.0100 train acc:99.65% val loss:0.5575 val acc:92.13%\n",
            "161/200 train loss:0.0062 train acc:99.80% val loss:0.4729 val acc:93.10%\n",
            "162/200 train loss:0.0087 train acc:99.72% val loss:0.4405 val acc:92.24%\n",
            "163/200 train loss:0.0091 train acc:99.71% val loss:0.4898 val acc:89.83%\n",
            "164/200 train loss:0.0123 train acc:99.59% val loss:0.5243 val acc:89.28%\n",
            "165/200 train loss:0.0035 train acc:99.89% val loss:0.2531 val acc:95.32%\n",
            "166/200 train loss:0.0049 train acc:99.84% val loss:0.2681 val acc:95.14%\n",
            "167/200 train loss:0.0099 train acc:99.65% val loss:0.4726 val acc:90.07%\n",
            "168/200 train loss:0.0153 train acc:99.60% val loss:0.2990 val acc:94.67%\n",
            "169/200 train loss:0.0068 train acc:99.80% val loss:0.2861 val acc:94.48%\n",
            "170/200 train loss:0.0052 train acc:99.85% val loss:0.4103 val acc:90.60%\n",
            "171/200 train loss:0.0099 train acc:99.69% val loss:0.8229 val acc:87.49%\n",
            "172/200 train loss:0.0093 train acc:99.69% val loss:0.2445 val acc:95.32%\n",
            "173/200 train loss:0.0025 train acc:99.95% val loss:0.2614 val acc:95.32%\n",
            "174/200 train loss:0.0100 train acc:99.69% val loss:0.5644 val acc:87.68%\n",
            "175/200 train loss:0.0140 train acc:99.54% val loss:0.3366 val acc:93.49%\n",
            "176/200 train loss:0.0056 train acc:99.83% val loss:0.2595 val acc:94.91%\n",
            "177/200 train loss:0.0095 train acc:99.67% val loss:0.3398 val acc:92.90%\n",
            "178/200 train loss:0.0107 train acc:99.64% val loss:0.2391 val acc:95.19%\n",
            "179/200 train loss:0.0033 train acc:99.90% val loss:0.2930 val acc:94.94%\n",
            "180/200 train loss:0.0050 train acc:99.83% val loss:0.3102 val acc:94.37%\n",
            "181/200 train loss:0.0110 train acc:99.65% val loss:0.4533 val acc:90.27%\n",
            "182/200 train loss:0.0080 train acc:99.73% val loss:0.3135 val acc:93.51%\n",
            "183/200 train loss:0.0102 train acc:99.64% val loss:0.4049 val acc:93.32%\n",
            "184/200 train loss:0.0067 train acc:99.77% val loss:0.2974 val acc:94.30%\n",
            "185/200 train loss:0.0074 train acc:99.76% val loss:0.2821 val acc:95.02%\n",
            "186/200 train loss:0.0069 train acc:99.75% val loss:0.3682 val acc:93.56%\n",
            "187/200 train loss:0.0060 train acc:99.77% val loss:0.2522 val acc:95.46%\n",
            "188/200 train loss:0.0088 train acc:99.71% val loss:0.2463 val acc:95.08%\n",
            "189/200 train loss:0.0144 train acc:99.52% val loss:0.2279 val acc:94.64%\n",
            "190/200 train loss:0.0084 train acc:99.73% val loss:0.2987 val acc:94.11%\n",
            "191/200 train loss:0.0051 train acc:99.85% val loss:0.2362 val acc:94.97%\n",
            "192/200 train loss:0.0090 train acc:99.71% val loss:0.3365 val acc:92.17%\n",
            "193/200 train loss:0.0056 train acc:99.82% val loss:0.3058 val acc:94.59%\n",
            "194/200 train loss:0.0045 train acc:99.84% val loss:0.2717 val acc:94.33%\n",
            "195/200 train loss:0.0102 train acc:99.66% val loss:0.2567 val acc:94.52%\n",
            "196/200 train loss:0.0033 train acc:99.91% val loss:0.2796 val acc:95.27%\n",
            "197/200 train loss:0.0047 train acc:99.83% val loss:0.3789 val acc:93.65%\n",
            "198/200 train loss:0.0098 train acc:99.66% val loss:0.2747 val acc:94.74%\n",
            "199/200 train loss:0.0083 train acc:99.71% val loss:0.2924 val acc:95.02%\n",
            "200/200 train loss:0.0105 train acc:99.65% val loss:0.4870 val acc:89.69%\n"
          ]
        }
      ],
      "source": [
        "from models import MobileNetV3_Large\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2'\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = MobileNetV3_Large(input_channel=8, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models import MobileNetV3_Large, mbv2_ca\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobileCA'\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = mbv2_ca(in_c=8, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile-ca{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tvLT-jVwyfwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kny_KG_1I7Y"
      },
      "source": [
        "## EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3vy1vjm1I7Y"
      },
      "outputs": [],
      "source": [
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = effnetv2_xl().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'efficient',\n",
        "                                              \"XL-{:.2f}.pth\".format(100 * correct_val/num_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGN88s7v1I7Z"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IRmiIp0hrkk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe1c41f-f4d9-4684-b9cc-c48048a38bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time:125.82060384750366sec\n"
          ]
        }
      ],
      "source": [
        "model_pth = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobile95.46.pth'\n",
        "\n",
        "model_pretrained = MobileNetV3_Large(input_channel=8, num_classes=4)\n",
        "model_pretrained.load_state_dict(torch.load(model_pth))\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "voting_dir = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/results'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/test_pub/audio'\n",
        "\n",
        "voting(audio_folder, voting_dir, model_pretrained, device, save_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2268
        },
        "id": "nv4DQKgjsHjV",
        "outputId": "bcb67b6c-1d33-447a-eaf4-0c8b5a48bd30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c2f79c1f-38ca-4872-85a7-857b42ca18d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_num</th>\n",
              "      <th>file</th>\n",
              "      <th>count_pred</th>\n",
              "      <th>final_pred</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000000</th>\n",
              "      <td>0</td>\n",
              "      <td>000000</td>\n",
              "      <td>[6, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000001</th>\n",
              "      <td>1</td>\n",
              "      <td>000001</td>\n",
              "      <td>[11, 16, 2, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000002</th>\n",
              "      <td>2</td>\n",
              "      <td>000002</td>\n",
              "      <td>[22, 39, 0, 4]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000003</th>\n",
              "      <td>3</td>\n",
              "      <td>000003</td>\n",
              "      <td>[40, 0, 0, 49]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000004</th>\n",
              "      <td>4</td>\n",
              "      <td>000004</td>\n",
              "      <td>[15, 0, 0, 22]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000223</th>\n",
              "      <td>223</td>\n",
              "      <td>000223</td>\n",
              "      <td>[61, 0, 0, 33]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000224</th>\n",
              "      <td>224</td>\n",
              "      <td>000224</td>\n",
              "      <td>[12, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000225</th>\n",
              "      <td>225</td>\n",
              "      <td>000225</td>\n",
              "      <td>[56, 0, 0, 23]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000226</th>\n",
              "      <td>226</td>\n",
              "      <td>000226</td>\n",
              "      <td>[19, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000227</th>\n",
              "      <td>227</td>\n",
              "      <td>000227</td>\n",
              "      <td>[25, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>228 rows  5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2f79c1f-38ca-4872-85a7-857b42ca18d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2f79c1f-38ca-4872-85a7-857b42ca18d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2f79c1f-38ca-4872-85a7-857b42ca18d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       data_num  ...                                               pred\n",
              "000000        0  ...                                 [0, 0, 0, 0, 0, 0]\n",
              "000001        1  ...  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "000002        2  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "000003        3  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, ...\n",
              "000004        4  ...  [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
              "...         ...  ...                                                ...\n",
              "000223      223  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, ...\n",
              "000224      224  ...               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000225      225  ...  [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...\n",
              "000226      226  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "000227      227  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[228 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "f = open(os.path.join(voting_dir, \"voting.json\"))\n",
        "vote_js = json.load(f)\n",
        "\n",
        "vote = pd.DataFrame(vote_js).T\n",
        "vote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vote.to_csv('vote.csv', index=False)"
      ],
      "metadata": {
        "id": "yfG4ArDttrFq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG3xQQqJoQpv",
        "outputId": "4c29b1c8-5005-4e50-8883-9b5659c4d5a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 100.00%\n"
          ]
        }
      ],
      "source": [
        "gt = pd.read_csv('files/train.csv')\n",
        "acc = np.sum(gt['filling_type'].to_numpy() == vote['final_pred'].to_numpy()) / len(gt['filling_type'])\n",
        "print('Acc: {:.2f}%'.format(100 * acc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aAtLh0sm1I7M"
      ],
      "name": "task2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e636d3e9ac1a4c7c8b43857197121142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ace71ac4aaa44b05918a69e1ddcfb00d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa0f261a420a47deb8ded85257414047",
              "IPY_MODEL_2920461c1b4d49099f4c47f809b92b24",
              "IPY_MODEL_cf1a671e92fd4910a6cada9562d6689d"
            ]
          }
        },
        "ace71ac4aaa44b05918a69e1ddcfb00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa0f261a420a47deb8ded85257414047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35cb128146ca465b9bdd8959f91214e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_727b5174d04649388c56d3a1b73d1a05"
          }
        },
        "2920461c1b4d49099f4c47f809b92b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_193e779009d64668be7dd9bf0307b5aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 31812,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 31812,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc8303a70e6c4592830ed02a3ccb48cd"
          }
        },
        "cf1a671e92fd4910a6cada9562d6689d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6eca861256a248b1867525eeeeb4a52a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 31812/31812 [01:22&lt;00:00, 341.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74eb35c35b6445039222f04d6d584f86"
          }
        },
        "35cb128146ca465b9bdd8959f91214e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "727b5174d04649388c56d3a1b73d1a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "193e779009d64668be7dd9bf0307b5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc8303a70e6c4592830ed02a3ccb48cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6eca861256a248b1867525eeeeb4a52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74eb35c35b6445039222f04d6d584f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}