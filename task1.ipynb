{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uy1XaYeYMs4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD5RtQIauafS",
        "outputId": "a28b92bc-6b4c-4677-d58b-9756156394ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jms_UNoAugPs",
        "outputId": "1be14748-356e-4b28-fcd9-b1df592e6c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yo9DNg9hy9x",
        "outputId": "f7aad011-f5de-460b-e28b-4e8e43f51e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.2-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQfv4bkWeyrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4904bb1d-bb55-46cd-d467-e61aba4317c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from torchinfo import summary\n",
        "from utils import AudioProcessing, audioPreprocessing, audioPreprocessing_t1, voting\n",
        "from models import Net, effnetv2_xl, MobileNetV3_Large, CNN_LSTM, mbv2_ca\n",
        "from dataset import MyLSTMDataset\n",
        "from helper import train_lstm, evaluate_audio\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "OK345_nrpxEU",
        "outputId": "62bb6921-4ec8-4c3d-8c0a-4369a382cde5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-09b00268-c13b-46c5-9cfb-aa62e0ef1115\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>container id</th>\n",
              "      <th>scenario</th>\n",
              "      <th>background</th>\n",
              "      <th>illumination</th>\n",
              "      <th>width at the top</th>\n",
              "      <th>width at the bottom</th>\n",
              "      <th>height</th>\n",
              "      <th>depth</th>\n",
              "      <th>container capacity</th>\n",
              "      <th>container mass</th>\n",
              "      <th>filling type</th>\n",
              "      <th>filling level</th>\n",
              "      <th>filling density</th>\n",
              "      <th>filling mass</th>\n",
              "      <th>object mass</th>\n",
              "      <th>handover starting frame</th>\n",
              "      <th>handover start timestamp</th>\n",
              "      <th>handover hand</th>\n",
              "      <th>action</th>\n",
              "      <th>nframes</th>\n",
              "      <th>folder_num</th>\n",
              "      <th>file_name</th>\n",
              "      <th>num</th>\n",
              "      <th>subject</th>\n",
              "      <th>filling_type</th>\n",
              "      <th>filling_level</th>\n",
              "      <th>back</th>\n",
              "      <th>light</th>\n",
              "      <th>camera_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82</td>\n",
              "      <td>76.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>291576</td>\n",
              "      <td>2</td>\n",
              "      <td>s2_fi2_fu1_b1_l0</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3209.397</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118483</td>\n",
              "      <td>7</td>\n",
              "      <td>s0_fi0_fu0_b0_l0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>572008</td>\n",
              "      <td>2</td>\n",
              "      <td>s0_fi3_fu1_b1_l0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.40</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1239.840</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141680</td>\n",
              "      <td>8</td>\n",
              "      <td>s0_fi0_fu0_b1_l0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>296.000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34</td>\n",
              "      <td>45.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138681</td>\n",
              "      <td>4</td>\n",
              "      <td>s1_fi1_fu1_b1_l0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09b00268-c13b-46c5-9cfb-aa62e0ef1115')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09b00268-c13b-46c5-9cfb-aa62e0ef1115 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09b00268-c13b-46c5-9cfb-aa62e0ef1115');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id  container id  scenario  background  ...  light  camera_id  start  end\n",
              "0   0             2         2           1  ...      0          2   0.75  3.5\n",
              "1   1             7         0           0  ...      0          2  -1.00 -1.0\n",
              "2   2             2         0           1  ...      0          2   3.40  6.5\n",
              "3   3             8         0           1  ...      0          2  -1.00 -1.0\n",
              "4   4             4         1           1  ...      0          2   0.75  1.8\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "gt = pd.read_csv('files/train.csv')\n",
        "gt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQysZ884ZNMv"
      },
      "outputs": [],
      "source": [
        "# efficient = '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios/efficient/XL-97.14.pth'\n",
        "# base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "# audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "# T2_mid_dir = os.path.join(base_path, 'T2_mid')\n",
        "# T2_pred_dir = os.path.join(base_path, 'T2_pred')\n",
        "# os.makedirs(T2_mid_dir,exist_ok=True)\n",
        "# os.makedirs(T2_pred_dir,exist_ok=True)\n",
        "\n",
        "# model = effnetv2_xl()\n",
        "# model.load_state_dict(torch.load(efficient))\n",
        "# model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# audioPreprocessing_t1(audio_folder, gt,T2_mid_dir, T2_pred_dir, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mobileNet = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobile95.46.pth'\n",
        "# base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "# audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "# T2_mid_dir = os.path.join(base_path, 'T2_mid')\n",
        "# T2_pred_dir = os.path.join(base_path, 'T2_pred')\n",
        "# os.makedirs(T2_mid_dir,exist_ok=True)\n",
        "# os.makedirs(T2_pred_dir,exist_ok=True)\n",
        "\n",
        "# model = MobileNetV3_Large(input_channel=8,num_classes=4)\n",
        "# model.load_state_dict(torch.load(mobileNet))\n",
        "# model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# audioPreprocessing_t1(audio_folder, gt,T2_mid_dir, T2_pred_dir, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "45c34a5a16294aed87ddbace2ebd3db8",
            "2a1fc65a65184d57a33432d405e78f2a",
            "cbe515daaa0844a49b653394f480062b",
            "0d39cd3e34fc431cb8ae06d272665a10",
            "7af9bbbb5c394e0f804992cfee29372b",
            "8bc4ba9ea5eb4aee996c6211acd23ebb",
            "be4eed98792443f0b919c7a9b505b3fe",
            "6b111da5122a46a598240bfbc9b891f1",
            "53142ff8983a4d838db5ad55cfe83ab4",
            "584a82a6faa84ad59e00673954b80240",
            "ec0dd012874849639c7e3a170e2ae3bf"
          ]
        },
        "id": "5makDy8lijZw",
        "outputId": "e6636559-f71a-414b-d1df-86da53051f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45c34a5a16294aed87ddbace2ebd3db8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "keDwJQxMXX-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobileNet = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobileCA/mobile-ca96.35.pth'\n",
        "base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task1/mobileCA/features'\n",
        "# audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "# T2_mid_dir = os.path.join(base_path, 'T2_mid')\n",
        "# T2_pred_dir = os.path.join(base_path, 'T2_pred')\n",
        "# os.makedirs(T2_mid_dir,exist_ok=True)\n",
        "# os.makedirs(T2_pred_dir,exist_ok=True)\n",
        "\n",
        "# model = mbv2_ca(in_c=8, num_classes=4)\n",
        "# model.load_state_dict(torch.load(mobileNet))\n",
        "# model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# audioPreprocessing_t1(audio_folder, gt,T2_mid_dir, T2_pred_dir, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "292ddece32df448b8a2d57427de8f5fe",
            "6e1d81abef9c44d3932022aa10cf0ac9",
            "38c016ad73364d8fab0cc78a7799a8f8",
            "3aa4bd646fb24546b24260d5a1ef7fcc",
            "a2158cb8927c4dad91713adef50624ab",
            "7793fc99332f494595ba26a43003fb65",
            "a6ff583cc9894b8596636e9b344389a0",
            "5e51791b70984af5b62e0ab30989064d",
            "e700068b2aa44ac7836f8d5623931d5b",
            "de1d4496ad44424ab00b32aa855d86ab",
            "58bd3edb50d4434c8d75e7d8a16d54bb"
          ]
        },
        "id": "un11PDxLL8Qo",
        "outputId": "a8bff16f-b7df-40af-a42f-591c7883bcd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "292ddece32df448b8a2d57427de8f5fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkp7QmaNuNLu"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuXtMEhvbQp",
        "outputId": "9d68a9a7-5563-4185-d9e5-59b8a744d7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "953\n",
            "/content/drive/MyDrive/COSRMAL_CHALLENGE/task1/mobileCA/features\n"
          ]
        }
      ],
      "source": [
        "myDataSet = MyLSTMDataset(base_path, gt['filling_level'].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAvpBiD6uNLv"
      },
      "source": [
        "## CNN_LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task1'\n",
        "\n",
        "model = CNN_LSTM(input_size=960).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train_lstm(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF6g5YPQkf8N",
        "outputId": "4bc0d14d-e09f-49ea-e435-c475e8e6a87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:1.0511 train acc:42.64% val loss:1.1367 val acc:39.00%\n",
            "2/200 train loss:0.9766 train acc:50.68% val loss:1.1989 val acc:50.00%\n",
            "3/200 train loss:0.9511 train acc:52.74% val loss:1.0476 val acc:52.00%\n",
            "4/200 train loss:0.9799 train acc:52.05% val loss:1.0866 val acc:45.00%\n",
            "5/200 train loss:0.9536 train acc:49.66% val loss:1.0163 val acc:40.00%\n",
            "6/200 train loss:0.9277 train acc:51.71% val loss:1.0453 val acc:48.00%\n",
            "7/200 train loss:0.9208 train acc:50.68% val loss:1.1194 val acc:41.00%\n",
            "8/200 train loss:0.8521 train acc:56.51% val loss:0.8218 val acc:67.00%\n",
            "9/200 train loss:0.8988 train acc:56.68% val loss:0.9008 val acc:57.00%\n",
            "10/200 train loss:0.9176 train acc:49.83% val loss:1.0848 val acc:51.00%\n",
            "11/200 train loss:0.8074 train acc:57.53% val loss:0.9559 val acc:63.00%\n",
            "12/200 train loss:0.7751 train acc:60.79% val loss:1.0486 val acc:62.00%\n",
            "13/200 train loss:0.7479 train acc:63.53% val loss:1.1202 val acc:52.00%\n",
            "14/200 train loss:0.6551 train acc:66.61% val loss:0.5124 val acc:67.00%\n",
            "15/200 train loss:0.7834 train acc:62.33% val loss:0.8030 val acc:58.00%\n",
            "16/200 train loss:0.5834 train acc:69.86% val loss:0.4645 val acc:70.00%\n",
            "17/200 train loss:0.6115 train acc:67.47% val loss:0.4539 val acc:72.00%\n",
            "18/200 train loss:0.8262 train acc:61.64% val loss:0.7855 val acc:60.00%\n",
            "19/200 train loss:0.6108 train acc:67.98% val loss:1.2064 val acc:55.00%\n",
            "20/200 train loss:0.7275 train acc:64.04% val loss:0.5550 val acc:68.00%\n",
            "21/200 train loss:0.6052 train acc:69.52% val loss:0.6270 val acc:72.00%\n",
            "22/200 train loss:0.6377 train acc:68.66% val loss:0.4888 val acc:72.00%\n",
            "23/200 train loss:0.5891 train acc:68.84% val loss:1.0086 val acc:66.00%\n",
            "24/200 train loss:0.6654 train acc:65.92% val loss:0.4360 val acc:79.00%\n",
            "25/200 train loss:0.6688 train acc:68.15% val loss:0.4049 val acc:80.00%\n",
            "26/200 train loss:0.6358 train acc:67.47% val loss:0.4411 val acc:72.00%\n",
            "27/200 train loss:0.7245 train acc:67.98% val loss:0.8839 val acc:67.00%\n",
            "28/200 train loss:0.6500 train acc:69.35% val loss:0.5838 val acc:61.00%\n",
            "29/200 train loss:0.5885 train acc:63.87% val loss:0.4275 val acc:77.00%\n",
            "30/200 train loss:0.5417 train acc:72.60% val loss:0.4257 val acc:78.00%\n",
            "31/200 train loss:0.6387 train acc:68.66% val loss:0.4212 val acc:74.00%\n",
            "32/200 train loss:0.5895 train acc:70.03% val loss:0.4310 val acc:75.00%\n",
            "33/200 train loss:0.5612 train acc:71.92% val loss:0.4516 val acc:78.00%\n",
            "34/200 train loss:0.5709 train acc:70.72% val loss:0.4158 val acc:77.00%\n",
            "35/200 train loss:0.5933 train acc:73.12% val loss:0.4255 val acc:78.00%\n",
            "36/200 train loss:0.5681 train acc:72.60% val loss:0.4100 val acc:80.00%\n",
            "37/200 train loss:0.5063 train acc:75.34% val loss:0.4255 val acc:76.00%\n",
            "38/200 train loss:0.6389 train acc:69.35% val loss:0.4607 val acc:75.00%\n",
            "39/200 train loss:0.5223 train acc:73.63% val loss:0.4031 val acc:78.00%\n",
            "40/200 train loss:0.5285 train acc:74.66% val loss:0.3895 val acc:81.00%\n",
            "41/200 train loss:0.5029 train acc:76.03% val loss:0.3941 val acc:78.00%\n",
            "42/200 train loss:0.5559 train acc:74.32% val loss:0.4424 val acc:78.00%\n",
            "43/200 train loss:0.5643 train acc:67.81% val loss:0.5036 val acc:62.00%\n",
            "44/200 train loss:0.5860 train acc:60.96% val loss:0.4033 val acc:79.00%\n",
            "45/200 train loss:0.5387 train acc:73.29% val loss:0.3937 val acc:78.00%\n",
            "46/200 train loss:0.4976 train acc:77.05% val loss:0.3976 val acc:81.00%\n",
            "47/200 train loss:0.5245 train acc:76.03% val loss:0.3848 val acc:81.00%\n",
            "48/200 train loss:0.5033 train acc:75.34% val loss:0.4040 val acc:77.00%\n",
            "49/200 train loss:0.5046 train acc:76.20% val loss:0.4393 val acc:75.00%\n",
            "50/200 train loss:0.5437 train acc:74.83% val loss:0.3895 val acc:80.00%\n",
            "51/200 train loss:0.5155 train acc:77.40% val loss:1.3786 val acc:67.00%\n",
            "52/200 train loss:0.6659 train acc:71.23% val loss:0.6799 val acc:74.00%\n",
            "53/200 train loss:0.5218 train acc:75.51% val loss:0.5103 val acc:69.00%\n",
            "54/200 train loss:0.5293 train acc:73.46% val loss:0.4325 val acc:76.00%\n",
            "55/200 train loss:0.5095 train acc:74.66% val loss:0.4120 val acc:74.00%\n",
            "56/200 train loss:0.5128 train acc:75.17% val loss:0.4117 val acc:80.00%\n",
            "57/200 train loss:0.4863 train acc:75.86% val loss:0.4039 val acc:78.00%\n",
            "58/200 train loss:0.4901 train acc:76.71% val loss:0.3948 val acc:80.00%\n",
            "59/200 train loss:0.5119 train acc:76.20% val loss:0.4290 val acc:74.00%\n",
            "60/200 train loss:0.6491 train acc:67.29% val loss:0.4404 val acc:73.00%\n",
            "61/200 train loss:0.6112 train acc:71.58% val loss:0.4195 val acc:80.00%\n",
            "62/200 train loss:0.5523 train acc:71.92% val loss:0.4099 val acc:82.00%\n",
            "63/200 train loss:0.5340 train acc:75.51% val loss:0.4788 val acc:80.00%\n",
            "64/200 train loss:0.5061 train acc:76.54% val loss:0.4296 val acc:81.00%\n",
            "65/200 train loss:0.4983 train acc:76.54% val loss:0.3827 val acc:80.00%\n",
            "66/200 train loss:0.4806 train acc:77.74% val loss:0.3972 val acc:79.00%\n",
            "67/200 train loss:0.4897 train acc:77.23% val loss:0.4277 val acc:76.00%\n",
            "68/200 train loss:0.5273 train acc:73.80% val loss:0.3783 val acc:82.00%\n",
            "69/200 train loss:0.4984 train acc:75.51% val loss:0.4140 val acc:77.00%\n",
            "70/200 train loss:0.5201 train acc:75.34% val loss:0.4141 val acc:77.00%\n",
            "71/200 train loss:0.4989 train acc:74.66% val loss:0.4522 val acc:68.00%\n",
            "72/200 train loss:0.4904 train acc:76.37% val loss:0.3739 val acc:82.00%\n",
            "73/200 train loss:0.5214 train acc:75.86% val loss:0.3761 val acc:81.00%\n",
            "74/200 train loss:0.5195 train acc:73.63% val loss:0.4418 val acc:71.00%\n",
            "75/200 train loss:0.5012 train acc:75.51% val loss:0.3835 val acc:81.00%\n",
            "76/200 train loss:0.5483 train acc:76.54% val loss:0.4238 val acc:81.00%\n",
            "77/200 train loss:0.4828 train acc:77.74% val loss:0.3887 val acc:79.00%\n",
            "78/200 train loss:0.5072 train acc:77.05% val loss:0.3872 val acc:81.00%\n",
            "79/200 train loss:0.5045 train acc:75.51% val loss:0.4292 val acc:74.00%\n",
            "80/200 train loss:0.4928 train acc:74.49% val loss:0.3928 val acc:80.00%\n",
            "81/200 train loss:0.4839 train acc:77.57% val loss:0.3674 val acc:81.00%\n",
            "82/200 train loss:0.4959 train acc:76.71% val loss:0.4294 val acc:75.00%\n",
            "83/200 train loss:0.4866 train acc:78.08% val loss:0.3986 val acc:80.00%\n",
            "84/200 train loss:0.4949 train acc:76.71% val loss:0.4475 val acc:73.00%\n",
            "85/200 train loss:0.4920 train acc:75.86% val loss:0.3734 val acc:83.00%\n",
            "86/200 train loss:0.5036 train acc:75.68% val loss:0.3978 val acc:76.00%\n",
            "87/200 train loss:0.4622 train acc:77.57% val loss:0.4535 val acc:72.00%\n",
            "88/200 train loss:0.4719 train acc:77.40% val loss:0.3779 val acc:80.00%\n",
            "89/200 train loss:0.4828 train acc:77.23% val loss:0.3728 val acc:82.00%\n",
            "90/200 train loss:0.4625 train acc:77.91% val loss:0.3722 val acc:81.00%\n",
            "91/200 train loss:0.4672 train acc:77.05% val loss:0.3668 val acc:81.00%\n",
            "92/200 train loss:0.4818 train acc:78.60% val loss:0.7349 val acc:74.00%\n",
            "93/200 train loss:0.5368 train acc:77.40% val loss:0.3907 val acc:79.00%\n",
            "94/200 train loss:0.4716 train acc:77.40% val loss:0.3652 val acc:81.00%\n",
            "95/200 train loss:0.4511 train acc:78.25% val loss:0.4358 val acc:78.00%\n",
            "96/200 train loss:0.4912 train acc:76.88% val loss:0.7583 val acc:63.00%\n",
            "97/200 train loss:0.4878 train acc:75.51% val loss:0.3869 val acc:77.00%\n",
            "98/200 train loss:0.5104 train acc:76.71% val loss:0.3795 val acc:81.00%\n",
            "99/200 train loss:0.6240 train acc:74.66% val loss:0.4121 val acc:79.00%\n",
            "100/200 train loss:0.5065 train acc:76.37% val loss:0.3978 val acc:78.00%\n",
            "101/200 train loss:0.4648 train acc:77.91% val loss:0.4294 val acc:76.00%\n",
            "102/200 train loss:0.5043 train acc:77.23% val loss:0.3887 val acc:79.00%\n",
            "103/200 train loss:0.4888 train acc:77.40% val loss:0.3563 val acc:84.00%\n",
            "104/200 train loss:0.4871 train acc:76.88% val loss:0.4035 val acc:79.00%\n",
            "105/200 train loss:0.4582 train acc:78.94% val loss:0.4172 val acc:77.00%\n",
            "106/200 train loss:0.4797 train acc:77.23% val loss:0.3992 val acc:79.00%\n",
            "107/200 train loss:0.5273 train acc:76.37% val loss:0.8056 val acc:71.00%\n",
            "108/200 train loss:0.7487 train acc:66.78% val loss:1.1892 val acc:64.00%\n",
            "109/200 train loss:0.6188 train acc:73.63% val loss:0.3870 val acc:81.00%\n",
            "110/200 train loss:0.4767 train acc:77.05% val loss:0.4396 val acc:72.00%\n",
            "111/200 train loss:0.4817 train acc:75.86% val loss:0.3731 val acc:80.00%\n",
            "112/200 train loss:0.4803 train acc:76.03% val loss:0.3869 val acc:81.00%\n",
            "113/200 train loss:0.4632 train acc:76.88% val loss:0.3907 val acc:80.00%\n",
            "114/200 train loss:0.4764 train acc:76.88% val loss:0.3934 val acc:79.00%\n",
            "115/200 train loss:0.4514 train acc:78.60% val loss:0.3630 val acc:81.00%\n",
            "116/200 train loss:0.4495 train acc:77.74% val loss:0.3659 val acc:81.00%\n",
            "117/200 train loss:0.4611 train acc:77.74% val loss:0.3869 val acc:81.00%\n",
            "118/200 train loss:0.5534 train acc:74.66% val loss:0.5527 val acc:73.00%\n",
            "119/200 train loss:0.4561 train acc:78.08% val loss:0.4101 val acc:78.00%\n",
            "120/200 train loss:0.4558 train acc:77.23% val loss:0.3628 val acc:82.00%\n",
            "121/200 train loss:0.4691 train acc:76.20% val loss:0.3946 val acc:77.00%\n",
            "122/200 train loss:0.4647 train acc:77.74% val loss:0.3778 val acc:80.00%\n",
            "123/200 train loss:0.4521 train acc:77.40% val loss:0.3765 val acc:81.00%\n",
            "124/200 train loss:0.5026 train acc:75.00% val loss:0.3968 val acc:78.00%\n",
            "125/200 train loss:0.5123 train acc:76.03% val loss:0.4032 val acc:77.00%\n",
            "126/200 train loss:0.4572 train acc:77.40% val loss:0.5355 val acc:73.00%\n",
            "127/200 train loss:0.4733 train acc:76.37% val loss:0.3726 val acc:81.00%\n",
            "128/200 train loss:0.4772 train acc:77.40% val loss:0.5178 val acc:76.00%\n",
            "129/200 train loss:0.4648 train acc:77.40% val loss:0.4232 val acc:82.00%\n",
            "130/200 train loss:0.4926 train acc:75.86% val loss:0.4101 val acc:75.00%\n",
            "131/200 train loss:0.4603 train acc:77.74% val loss:0.3622 val acc:81.00%\n",
            "132/200 train loss:0.4606 train acc:76.71% val loss:0.4582 val acc:72.00%\n",
            "133/200 train loss:0.4425 train acc:78.94% val loss:0.3799 val acc:80.00%\n",
            "134/200 train loss:0.4718 train acc:78.25% val loss:0.3814 val acc:81.00%\n",
            "135/200 train loss:0.4719 train acc:74.49% val loss:0.3619 val acc:81.00%\n",
            "136/200 train loss:0.5269 train acc:77.40% val loss:0.5973 val acc:78.00%\n",
            "137/200 train loss:0.4729 train acc:77.57% val loss:0.4253 val acc:76.00%\n",
            "138/200 train loss:0.4547 train acc:77.74% val loss:0.4491 val acc:73.00%\n",
            "139/200 train loss:0.4633 train acc:77.74% val loss:0.3594 val acc:80.00%\n",
            "140/200 train loss:0.4526 train acc:76.88% val loss:0.3758 val acc:81.00%\n",
            "141/200 train loss:0.4462 train acc:78.08% val loss:0.4154 val acc:78.00%\n",
            "142/200 train loss:0.4538 train acc:77.05% val loss:0.3803 val acc:80.00%\n",
            "143/200 train loss:0.4315 train acc:78.77% val loss:0.4255 val acc:75.00%\n",
            "144/200 train loss:0.4723 train acc:75.34% val loss:0.3854 val acc:78.00%\n",
            "145/200 train loss:0.4510 train acc:77.05% val loss:0.4557 val acc:73.00%\n",
            "146/200 train loss:0.4654 train acc:77.23% val loss:0.3835 val acc:80.00%\n",
            "147/200 train loss:0.5373 train acc:78.25% val loss:0.3966 val acc:79.00%\n",
            "148/200 train loss:0.5168 train acc:76.71% val loss:0.3978 val acc:79.00%\n",
            "149/200 train loss:0.4561 train acc:77.57% val loss:0.3863 val acc:80.00%\n",
            "150/200 train loss:0.4618 train acc:77.57% val loss:0.3735 val acc:80.00%\n",
            "151/200 train loss:0.4593 train acc:76.37% val loss:0.4012 val acc:78.00%\n",
            "152/200 train loss:0.4721 train acc:77.91% val loss:0.3612 val acc:81.00%\n",
            "153/200 train loss:0.4502 train acc:77.57% val loss:0.3809 val acc:80.00%\n",
            "154/200 train loss:0.4426 train acc:78.25% val loss:0.3610 val acc:81.00%\n",
            "155/200 train loss:0.4483 train acc:76.71% val loss:0.4063 val acc:77.00%\n",
            "156/200 train loss:0.4416 train acc:77.57% val loss:0.3849 val acc:78.00%\n",
            "157/200 train loss:0.4810 train acc:77.05% val loss:0.3610 val acc:83.00%\n",
            "158/200 train loss:0.4483 train acc:78.08% val loss:0.4178 val acc:76.00%\n",
            "159/200 train loss:0.4491 train acc:77.57% val loss:0.3831 val acc:80.00%\n",
            "160/200 train loss:0.4496 train acc:78.08% val loss:0.3905 val acc:80.00%\n",
            "161/200 train loss:0.4814 train acc:78.08% val loss:0.4512 val acc:74.00%\n",
            "162/200 train loss:0.4680 train acc:78.08% val loss:0.3807 val acc:79.00%\n",
            "163/200 train loss:0.4523 train acc:77.91% val loss:0.3535 val acc:84.00%\n",
            "164/200 train loss:0.4569 train acc:78.60% val loss:0.3636 val acc:79.00%\n",
            "165/200 train loss:0.4531 train acc:78.94% val loss:0.4450 val acc:74.00%\n",
            "166/200 train loss:0.4673 train acc:77.74% val loss:0.3816 val acc:80.00%\n",
            "167/200 train loss:0.4563 train acc:77.74% val loss:0.4078 val acc:76.00%\n",
            "168/200 train loss:0.4215 train acc:79.45% val loss:0.3658 val acc:81.00%\n",
            "169/200 train loss:0.4677 train acc:77.57% val loss:0.3875 val acc:79.00%\n",
            "170/200 train loss:0.4269 train acc:79.11% val loss:0.3831 val acc:80.00%\n",
            "171/200 train loss:0.4264 train acc:77.74% val loss:0.3843 val acc:79.00%\n",
            "172/200 train loss:0.4307 train acc:78.42% val loss:0.4585 val acc:74.00%\n",
            "173/200 train loss:0.4948 train acc:75.68% val loss:0.3512 val acc:81.00%\n",
            "174/200 train loss:0.4709 train acc:76.71% val loss:0.3730 val acc:79.00%\n",
            "175/200 train loss:0.4414 train acc:79.11% val loss:0.3694 val acc:77.00%\n",
            "176/200 train loss:0.4275 train acc:79.28% val loss:0.3974 val acc:77.00%\n",
            "177/200 train loss:0.4436 train acc:78.60% val loss:0.4048 val acc:79.00%\n",
            "178/200 train loss:0.4438 train acc:78.60% val loss:0.4634 val acc:75.00%\n",
            "179/200 train loss:0.5230 train acc:76.88% val loss:0.3629 val acc:79.00%\n",
            "180/200 train loss:0.4623 train acc:79.45% val loss:0.3991 val acc:81.00%\n",
            "181/200 train loss:0.4571 train acc:78.25% val loss:0.3697 val acc:82.00%\n",
            "182/200 train loss:0.4412 train acc:77.23% val loss:0.4226 val acc:76.00%\n",
            "183/200 train loss:0.4540 train acc:77.74% val loss:0.3834 val acc:79.00%\n",
            "184/200 train loss:0.4257 train acc:78.60% val loss:0.3767 val acc:79.00%\n",
            "185/200 train loss:0.4215 train acc:78.42% val loss:0.4624 val acc:75.00%\n",
            "186/200 train loss:0.4814 train acc:76.37% val loss:0.4257 val acc:74.00%\n",
            "187/200 train loss:0.4766 train acc:77.23% val loss:0.4954 val acc:79.00%\n",
            "188/200 train loss:0.4260 train acc:78.94% val loss:0.3897 val acc:80.00%\n",
            "189/200 train loss:0.4768 train acc:78.60% val loss:0.4778 val acc:76.00%\n",
            "190/200 train loss:0.4471 train acc:78.25% val loss:0.3754 val acc:81.00%\n",
            "191/200 train loss:0.5032 train acc:78.42% val loss:0.4072 val acc:81.00%\n",
            "192/200 train loss:0.4568 train acc:77.23% val loss:0.3580 val acc:83.00%\n",
            "193/200 train loss:0.4282 train acc:79.62% val loss:0.3776 val acc:81.00%\n",
            "194/200 train loss:0.4311 train acc:76.88% val loss:0.3746 val acc:81.00%\n",
            "195/200 train loss:0.4400 train acc:78.42% val loss:0.3689 val acc:83.00%\n",
            "196/200 train loss:0.4832 train acc:79.45% val loss:0.3720 val acc:81.00%\n",
            "197/200 train loss:0.4672 train acc:78.25% val loss:0.4123 val acc:78.00%\n",
            "198/200 train loss:0.4261 train acc:79.28% val loss:0.3753 val acc:80.00%\n",
            "199/200 train loss:0.4358 train acc:79.11% val loss:0.3689 val acc:82.00%\n",
            "200/200 train loss:0.4897 train acc:76.20% val loss:0.4012 val acc:80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task1/mobileCA'\n",
        "\n",
        "model = CNN_LSTM(input_size=1280).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train_lstm(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rTWKsc5xLzSa",
        "outputId": "f22ff04e-19c0-4fd6-8d7d-f33531a3f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:1.0670 train acc:41.61% val loss:1.0300 val acc:45.00%\n",
            "2/200 train loss:1.0088 train acc:41.61% val loss:1.0257 val acc:47.00%\n",
            "3/200 train loss:0.9616 train acc:49.32% val loss:0.9984 val acc:54.00%\n",
            "4/200 train loss:0.9548 train acc:50.51% val loss:0.9713 val acc:44.00%\n",
            "5/200 train loss:0.8514 train acc:43.84% val loss:0.8110 val acc:45.00%\n",
            "6/200 train loss:0.7740 train acc:48.29% val loss:0.7674 val acc:55.00%\n",
            "7/200 train loss:0.7396 train acc:57.53% val loss:0.6074 val acc:55.00%\n",
            "8/200 train loss:0.6058 train acc:60.79% val loss:0.5468 val acc:69.00%\n",
            "9/200 train loss:0.5673 train acc:66.10% val loss:0.4956 val acc:70.00%\n",
            "10/200 train loss:0.5844 train acc:64.90% val loss:0.5168 val acc:61.00%\n",
            "11/200 train loss:0.5681 train acc:67.47% val loss:0.5582 val acc:69.00%\n",
            "12/200 train loss:0.6361 train acc:63.87% val loss:0.5245 val acc:70.00%\n",
            "13/200 train loss:0.5825 train acc:67.29% val loss:0.5362 val acc:65.00%\n",
            "14/200 train loss:0.5757 train acc:67.29% val loss:0.5545 val acc:63.00%\n",
            "15/200 train loss:0.5766 train acc:67.98% val loss:0.6078 val acc:72.00%\n",
            "16/200 train loss:0.6289 train acc:67.29% val loss:0.5427 val acc:64.00%\n",
            "17/200 train loss:0.5528 train acc:68.15% val loss:0.5458 val acc:69.00%\n",
            "18/200 train loss:0.5506 train acc:69.52% val loss:0.5158 val acc:67.00%\n",
            "19/200 train loss:0.5707 train acc:64.73% val loss:0.5263 val acc:64.00%\n",
            "20/200 train loss:0.5597 train acc:67.81% val loss:0.5315 val acc:69.00%\n",
            "21/200 train loss:0.5348 train acc:70.72% val loss:0.4965 val acc:70.00%\n",
            "22/200 train loss:0.5550 train acc:68.15% val loss:0.5196 val acc:63.00%\n",
            "23/200 train loss:0.5844 train acc:63.87% val loss:0.5324 val acc:63.00%\n",
            "24/200 train loss:0.5624 train acc:61.99% val loss:0.4490 val acc:77.00%\n",
            "25/200 train loss:0.5672 train acc:68.84% val loss:0.5175 val acc:69.00%\n",
            "26/200 train loss:0.5687 train acc:65.41% val loss:0.4696 val acc:75.00%\n",
            "27/200 train loss:0.5287 train acc:71.58% val loss:0.5539 val acc:59.00%\n",
            "28/200 train loss:0.5631 train acc:67.12% val loss:0.4987 val acc:67.00%\n",
            "29/200 train loss:0.5379 train acc:70.21% val loss:0.5265 val acc:63.00%\n",
            "30/200 train loss:0.5664 train acc:64.90% val loss:0.5172 val acc:63.00%\n",
            "31/200 train loss:0.5665 train acc:63.01% val loss:0.5315 val acc:64.00%\n",
            "32/200 train loss:0.5592 train acc:69.01% val loss:0.5057 val acc:68.00%\n",
            "33/200 train loss:0.5449 train acc:68.84% val loss:0.5061 val acc:68.00%\n",
            "34/200 train loss:0.5566 train acc:67.98% val loss:0.4831 val acc:72.00%\n",
            "35/200 train loss:0.5711 train acc:64.73% val loss:0.5196 val acc:70.00%\n",
            "36/200 train loss:0.5294 train acc:69.69% val loss:0.4965 val acc:68.00%\n",
            "37/200 train loss:0.5327 train acc:71.40% val loss:0.4847 val acc:74.00%\n",
            "38/200 train loss:0.5291 train acc:71.75% val loss:0.5156 val acc:68.00%\n",
            "39/200 train loss:0.5465 train acc:69.69% val loss:0.5661 val acc:67.00%\n",
            "40/200 train loss:0.5401 train acc:70.38% val loss:0.4904 val acc:70.00%\n",
            "41/200 train loss:0.6889 train acc:67.81% val loss:0.5000 val acc:69.00%\n",
            "42/200 train loss:0.5001 train acc:76.20% val loss:0.5563 val acc:68.00%\n",
            "43/200 train loss:0.5391 train acc:69.52% val loss:0.5038 val acc:71.00%\n",
            "44/200 train loss:0.5297 train acc:70.72% val loss:0.4945 val acc:70.00%\n",
            "45/200 train loss:0.5069 train acc:72.95% val loss:0.5056 val acc:70.00%\n",
            "46/200 train loss:0.5006 train acc:72.77% val loss:0.5055 val acc:69.00%\n",
            "47/200 train loss:0.5261 train acc:69.01% val loss:0.5334 val acc:70.00%\n",
            "48/200 train loss:0.4919 train acc:74.83% val loss:0.5056 val acc:62.00%\n",
            "49/200 train loss:0.4910 train acc:74.83% val loss:0.5375 val acc:72.00%\n",
            "50/200 train loss:0.4996 train acc:73.97% val loss:0.4990 val acc:69.00%\n",
            "51/200 train loss:0.4958 train acc:73.46% val loss:0.4792 val acc:75.00%\n",
            "52/200 train loss:0.4969 train acc:75.00% val loss:0.4939 val acc:73.00%\n",
            "53/200 train loss:0.5097 train acc:73.29% val loss:0.4968 val acc:65.00%\n",
            "54/200 train loss:0.5019 train acc:75.34% val loss:0.4698 val acc:73.00%\n",
            "55/200 train loss:0.5012 train acc:74.49% val loss:0.4748 val acc:76.00%\n",
            "56/200 train loss:0.4908 train acc:74.32% val loss:0.4547 val acc:75.00%\n",
            "57/200 train loss:0.4925 train acc:75.51% val loss:0.4692 val acc:77.00%\n",
            "58/200 train loss:0.4903 train acc:76.54% val loss:0.4450 val acc:78.00%\n",
            "59/200 train loss:0.4923 train acc:75.17% val loss:0.4348 val acc:74.00%\n",
            "60/200 train loss:0.5195 train acc:75.68% val loss:0.4591 val acc:75.00%\n",
            "61/200 train loss:0.5052 train acc:73.80% val loss:0.4555 val acc:76.00%\n",
            "62/200 train loss:0.4896 train acc:76.03% val loss:0.4487 val acc:72.00%\n",
            "63/200 train loss:0.4728 train acc:77.23% val loss:0.4491 val acc:77.00%\n",
            "64/200 train loss:0.4731 train acc:77.40% val loss:0.4623 val acc:76.00%\n",
            "65/200 train loss:0.4718 train acc:76.20% val loss:0.4550 val acc:78.00%\n",
            "66/200 train loss:0.4992 train acc:76.88% val loss:0.4330 val acc:77.00%\n",
            "67/200 train loss:0.4629 train acc:75.86% val loss:0.4327 val acc:78.00%\n",
            "68/200 train loss:0.5082 train acc:72.77% val loss:0.4638 val acc:75.00%\n",
            "69/200 train loss:0.4726 train acc:76.37% val loss:0.4455 val acc:73.00%\n",
            "70/200 train loss:0.4719 train acc:77.57% val loss:0.4529 val acc:76.00%\n",
            "71/200 train loss:0.4708 train acc:77.74% val loss:0.4617 val acc:75.00%\n",
            "72/200 train loss:0.4796 train acc:75.51% val loss:0.4605 val acc:76.00%\n",
            "73/200 train loss:0.4721 train acc:77.05% val loss:0.4460 val acc:77.00%\n",
            "74/200 train loss:0.4868 train acc:75.00% val loss:0.4626 val acc:74.00%\n",
            "75/200 train loss:0.4724 train acc:77.05% val loss:0.4465 val acc:74.00%\n",
            "76/200 train loss:0.4697 train acc:76.54% val loss:0.4477 val acc:76.00%\n",
            "77/200 train loss:0.4569 train acc:76.88% val loss:0.4649 val acc:71.00%\n",
            "78/200 train loss:0.4435 train acc:78.42% val loss:0.4705 val acc:73.00%\n",
            "79/200 train loss:0.4636 train acc:76.71% val loss:0.4568 val acc:75.00%\n",
            "80/200 train loss:0.4649 train acc:77.91% val loss:0.4450 val acc:75.00%\n",
            "81/200 train loss:0.4294 train acc:79.28% val loss:0.4348 val acc:77.00%\n",
            "82/200 train loss:0.4558 train acc:77.57% val loss:0.5070 val acc:75.00%\n",
            "83/200 train loss:0.4885 train acc:77.57% val loss:0.4402 val acc:78.00%\n",
            "84/200 train loss:0.4482 train acc:79.11% val loss:0.4379 val acc:77.00%\n",
            "85/200 train loss:0.4374 train acc:79.97% val loss:0.4212 val acc:75.00%\n",
            "86/200 train loss:0.4448 train acc:78.60% val loss:0.4648 val acc:72.00%\n",
            "87/200 train loss:0.4606 train acc:76.88% val loss:0.4052 val acc:77.00%\n",
            "88/200 train loss:0.4306 train acc:79.97% val loss:0.4448 val acc:75.00%\n",
            "89/200 train loss:0.4364 train acc:77.91% val loss:0.4117 val acc:77.00%\n",
            "90/200 train loss:0.4362 train acc:78.60% val loss:0.4471 val acc:74.00%\n",
            "91/200 train loss:0.4358 train acc:78.42% val loss:0.4008 val acc:80.00%\n",
            "92/200 train loss:0.4591 train acc:77.91% val loss:0.4297 val acc:76.00%\n",
            "93/200 train loss:0.4244 train acc:79.45% val loss:0.4516 val acc:76.00%\n",
            "94/200 train loss:0.4421 train acc:76.88% val loss:0.4455 val acc:74.00%\n",
            "95/200 train loss:0.4473 train acc:78.60% val loss:0.4092 val acc:78.00%\n",
            "96/200 train loss:0.4284 train acc:79.97% val loss:0.4326 val acc:77.00%\n",
            "97/200 train loss:0.4523 train acc:78.42% val loss:0.4110 val acc:79.00%\n",
            "98/200 train loss:0.4290 train acc:79.62% val loss:0.5113 val acc:69.00%\n",
            "99/200 train loss:0.4805 train acc:78.94% val loss:0.4139 val acc:77.00%\n",
            "100/200 train loss:0.4286 train acc:79.11% val loss:0.4362 val acc:76.00%\n",
            "101/200 train loss:0.4217 train acc:79.79% val loss:0.3886 val acc:78.00%\n",
            "102/200 train loss:0.4054 train acc:79.45% val loss:0.4214 val acc:76.00%\n",
            "103/200 train loss:0.4199 train acc:79.62% val loss:0.3747 val acc:78.00%\n",
            "104/200 train loss:0.4106 train acc:79.79% val loss:0.3933 val acc:80.00%\n",
            "105/200 train loss:0.4255 train acc:78.94% val loss:0.4285 val acc:79.00%\n",
            "106/200 train loss:0.4229 train acc:78.60% val loss:0.3832 val acc:78.00%\n",
            "107/200 train loss:0.4291 train acc:78.42% val loss:0.4281 val acc:76.00%\n",
            "108/200 train loss:0.4286 train acc:78.94% val loss:0.3944 val acc:77.00%\n",
            "109/200 train loss:0.4334 train acc:78.08% val loss:0.3826 val acc:77.00%\n",
            "110/200 train loss:0.4203 train acc:78.42% val loss:0.3943 val acc:80.00%\n",
            "111/200 train loss:0.4142 train acc:78.25% val loss:0.4079 val acc:76.00%\n",
            "112/200 train loss:0.4136 train acc:79.62% val loss:0.4053 val acc:77.00%\n",
            "113/200 train loss:0.4218 train acc:79.79% val loss:0.4013 val acc:78.00%\n",
            "114/200 train loss:0.4186 train acc:79.28% val loss:0.4020 val acc:78.00%\n",
            "115/200 train loss:0.4220 train acc:79.79% val loss:0.4228 val acc:74.00%\n",
            "116/200 train loss:0.4147 train acc:79.79% val loss:0.4200 val acc:75.00%\n",
            "117/200 train loss:0.4043 train acc:79.62% val loss:0.4192 val acc:75.00%\n",
            "118/200 train loss:0.4223 train acc:78.08% val loss:0.4067 val acc:76.00%\n",
            "119/200 train loss:0.4022 train acc:79.62% val loss:0.4110 val acc:75.00%\n",
            "120/200 train loss:0.4574 train acc:79.28% val loss:0.4007 val acc:77.00%\n",
            "121/200 train loss:0.4010 train acc:78.77% val loss:0.3762 val acc:80.00%\n",
            "122/200 train loss:0.4051 train acc:78.60% val loss:0.4407 val acc:80.00%\n",
            "123/200 train loss:0.4215 train acc:77.57% val loss:0.4348 val acc:77.00%\n",
            "124/200 train loss:0.4132 train acc:77.91% val loss:0.3974 val acc:79.00%\n",
            "125/200 train loss:0.3934 train acc:78.94% val loss:0.4297 val acc:75.00%\n",
            "126/200 train loss:0.4135 train acc:79.11% val loss:0.4160 val acc:76.00%\n",
            "127/200 train loss:0.4303 train acc:79.11% val loss:0.3916 val acc:80.00%\n",
            "128/200 train loss:0.4174 train acc:78.08% val loss:0.4011 val acc:76.00%\n",
            "129/200 train loss:0.3897 train acc:80.82% val loss:0.3801 val acc:80.00%\n",
            "130/200 train loss:0.4087 train acc:79.62% val loss:0.4099 val acc:79.00%\n",
            "131/200 train loss:0.4040 train acc:79.79% val loss:0.4071 val acc:76.00%\n",
            "132/200 train loss:0.3827 train acc:82.36% val loss:0.4211 val acc:77.00%\n",
            "133/200 train loss:0.4033 train acc:79.79% val loss:0.4183 val acc:77.00%\n",
            "134/200 train loss:0.4523 train acc:79.45% val loss:0.3918 val acc:81.00%\n",
            "135/200 train loss:0.4073 train acc:78.60% val loss:0.4838 val acc:67.00%\n",
            "136/200 train loss:0.4268 train acc:81.16% val loss:0.4035 val acc:80.00%\n",
            "137/200 train loss:0.4000 train acc:80.65% val loss:0.3987 val acc:76.00%\n",
            "138/200 train loss:0.4228 train acc:78.60% val loss:0.4057 val acc:79.00%\n",
            "139/200 train loss:0.4143 train acc:80.31% val loss:0.4106 val acc:77.00%\n",
            "140/200 train loss:0.4183 train acc:78.94% val loss:0.3965 val acc:76.00%\n",
            "141/200 train loss:0.3936 train acc:80.31% val loss:0.4104 val acc:77.00%\n",
            "142/200 train loss:0.4028 train acc:79.45% val loss:0.4383 val acc:75.00%\n",
            "143/200 train loss:0.3926 train acc:80.99% val loss:0.4482 val acc:76.00%\n",
            "144/200 train loss:0.3949 train acc:79.97% val loss:0.4359 val acc:77.00%\n",
            "145/200 train loss:0.3919 train acc:81.34% val loss:0.4058 val acc:77.00%\n",
            "146/200 train loss:0.4149 train acc:78.42% val loss:0.3817 val acc:77.00%\n",
            "147/200 train loss:0.3905 train acc:80.48% val loss:0.3928 val acc:79.00%\n",
            "148/200 train loss:0.3907 train acc:80.82% val loss:0.4028 val acc:78.00%\n",
            "149/200 train loss:0.3999 train acc:80.31% val loss:0.4289 val acc:76.00%\n",
            "150/200 train loss:0.3816 train acc:79.79% val loss:0.3994 val acc:78.00%\n",
            "151/200 train loss:0.3921 train acc:80.99% val loss:0.4364 val acc:77.00%\n",
            "152/200 train loss:0.3731 train acc:81.16% val loss:0.4489 val acc:75.00%\n",
            "153/200 train loss:0.3800 train acc:82.53% val loss:0.4455 val acc:72.00%\n",
            "154/200 train loss:0.3877 train acc:81.34% val loss:0.4145 val acc:76.00%\n",
            "155/200 train loss:0.3910 train acc:79.79% val loss:0.4316 val acc:76.00%\n",
            "156/200 train loss:0.4119 train acc:78.94% val loss:0.4467 val acc:80.00%\n",
            "157/200 train loss:0.3809 train acc:81.85% val loss:0.3794 val acc:81.00%\n",
            "158/200 train loss:0.3702 train acc:80.48% val loss:0.3994 val acc:79.00%\n",
            "159/200 train loss:0.3803 train acc:80.99% val loss:0.4002 val acc:77.00%\n",
            "160/200 train loss:0.3970 train acc:79.11% val loss:0.3998 val acc:79.00%\n",
            "161/200 train loss:0.3803 train acc:81.51% val loss:0.4293 val acc:77.00%\n",
            "162/200 train loss:0.3851 train acc:81.68% val loss:0.4180 val acc:77.00%\n",
            "163/200 train loss:0.3849 train acc:80.31% val loss:0.4085 val acc:80.00%\n",
            "164/200 train loss:0.4012 train acc:78.94% val loss:0.4199 val acc:79.00%\n",
            "165/200 train loss:0.3706 train acc:82.88% val loss:0.4523 val acc:73.00%\n",
            "166/200 train loss:0.3991 train acc:81.16% val loss:0.4194 val acc:75.00%\n",
            "167/200 train loss:0.3897 train acc:80.31% val loss:0.3827 val acc:77.00%\n",
            "168/200 train loss:0.3794 train acc:81.68% val loss:0.3997 val acc:83.00%\n",
            "169/200 train loss:0.3911 train acc:80.14% val loss:0.4162 val acc:77.00%\n",
            "170/200 train loss:0.4084 train acc:78.25% val loss:0.4202 val acc:76.00%\n",
            "171/200 train loss:0.3899 train acc:81.16% val loss:0.4173 val acc:79.00%\n",
            "172/200 train loss:0.4144 train acc:80.99% val loss:0.3949 val acc:81.00%\n",
            "173/200 train loss:0.3908 train acc:82.53% val loss:0.4027 val acc:79.00%\n",
            "174/200 train loss:0.3678 train acc:79.79% val loss:0.4217 val acc:72.00%\n",
            "175/200 train loss:0.3804 train acc:82.36% val loss:0.4269 val acc:78.00%\n",
            "176/200 train loss:0.3600 train acc:79.97% val loss:0.3910 val acc:75.00%\n",
            "177/200 train loss:0.3579 train acc:83.56% val loss:0.4159 val acc:81.00%\n",
            "178/200 train loss:0.3661 train acc:81.68% val loss:0.4170 val acc:74.00%\n",
            "179/200 train loss:0.3397 train acc:82.53% val loss:0.4467 val acc:73.00%\n",
            "180/200 train loss:0.3809 train acc:80.31% val loss:0.4248 val acc:80.00%\n",
            "181/200 train loss:0.3614 train acc:80.82% val loss:0.4798 val acc:71.00%\n",
            "182/200 train loss:0.3832 train acc:81.85% val loss:0.4280 val acc:77.00%\n",
            "183/200 train loss:0.3922 train acc:79.28% val loss:0.3978 val acc:78.00%\n",
            "184/200 train loss:0.3508 train acc:82.02% val loss:0.3784 val acc:78.00%\n",
            "185/200 train loss:0.3662 train acc:80.99% val loss:0.3934 val acc:80.00%\n",
            "186/200 train loss:0.3469 train acc:80.65% val loss:0.4326 val acc:77.00%\n",
            "187/200 train loss:0.3955 train acc:80.65% val loss:0.4099 val acc:77.00%\n",
            "188/200 train loss:0.3838 train acc:80.48% val loss:0.4371 val acc:78.00%\n",
            "189/200 train loss:0.3451 train acc:82.53% val loss:0.3854 val acc:80.00%\n",
            "190/200 train loss:0.3751 train acc:82.71% val loss:0.4091 val acc:75.00%\n",
            "191/200 train loss:0.3377 train acc:82.36% val loss:0.3795 val acc:81.00%\n",
            "192/200 train loss:0.3349 train acc:83.05% val loss:0.4877 val acc:74.00%\n",
            "193/200 train loss:0.3613 train acc:83.05% val loss:0.4014 val acc:78.00%\n",
            "194/200 train loss:0.3444 train acc:83.73% val loss:0.4276 val acc:76.00%\n",
            "195/200 train loss:0.3534 train acc:82.19% val loss:0.4442 val acc:75.00%\n",
            "196/200 train loss:0.3337 train acc:83.05% val loss:0.4477 val acc:75.00%\n",
            "197/200 train loss:0.3392 train acc:84.42% val loss:0.4116 val acc:76.00%\n",
            "198/200 train loss:0.3426 train acc:82.71% val loss:0.4470 val acc:75.00%\n",
            "199/200 train loss:0.3747 train acc:80.14% val loss:0.4588 val acc:75.00%\n",
            "200/200 train loss:0.3533 train acc:82.53% val loss:0.4265 val acc:77.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejbKNrd_wi2T",
        "outputId": "b8e1ff1c-305d-4db7-ba03-4095a0506f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 train loss:1.0452 train acc:46.40% \n",
            "Epoch 1/200 val loss:0.9191 val acc:41.00% \n",
            "Epoch 2/200 train loss:0.9919 train acc:50.00% \n",
            "Epoch 2/200 val loss:0.9260 val acc:61.00% \n",
            "Epoch 3/200 train loss:0.9881 train acc:49.14% \n",
            "Epoch 3/200 val loss:0.8794 val acc:58.00% \n",
            "Epoch 4/200 train loss:0.9932 train acc:51.54% \n",
            "Epoch 4/200 val loss:0.8525 val acc:66.00% \n",
            "Epoch 5/200 train loss:0.9659 train acc:49.66% \n",
            "Epoch 5/200 val loss:0.8356 val acc:60.00% \n",
            "Epoch 6/200 train loss:0.8339 train acc:46.92% \n",
            "Epoch 6/200 val loss:0.7376 val acc:67.00% \n",
            "Epoch 7/200 train loss:0.7594 train acc:64.21% \n",
            "Epoch 7/200 val loss:0.6884 val acc:63.00% \n",
            "Epoch 8/200 train loss:0.6278 train acc:68.32% \n",
            "Epoch 8/200 val loss:0.5749 val acc:76.00% \n",
            "Epoch 9/200 train loss:0.6022 train acc:63.36% \n",
            "Epoch 9/200 val loss:0.5800 val acc:73.00% \n",
            "Epoch 10/200 train loss:0.5727 train acc:64.38% \n",
            "Epoch 10/200 val loss:0.5843 val acc:73.00% \n",
            "Epoch 11/200 train loss:0.5532 train acc:64.73% \n",
            "Epoch 11/200 val loss:0.5212 val acc:79.00% \n",
            "Epoch 12/200 train loss:0.5567 train acc:68.15% \n",
            "Epoch 12/200 val loss:0.5119 val acc:77.00% \n",
            "Epoch 13/200 train loss:0.6193 train acc:69.52% \n",
            "Epoch 13/200 val loss:0.5090 val acc:73.00% \n",
            "Epoch 14/200 train loss:0.5685 train acc:67.12% \n",
            "Epoch 14/200 val loss:0.4821 val acc:82.00% \n",
            "Epoch 15/200 train loss:0.5788 train acc:69.01% \n",
            "Epoch 15/200 val loss:0.4596 val acc:82.00% \n",
            "Epoch 16/200 train loss:0.5301 train acc:72.77% \n",
            "Epoch 16/200 val loss:0.4868 val acc:80.00% \n",
            "Epoch 17/200 train loss:0.5325 train acc:71.40% \n",
            "Epoch 17/200 val loss:0.5664 val acc:76.00% \n",
            "Epoch 18/200 train loss:0.7241 train acc:66.95% \n",
            "Epoch 18/200 val loss:0.4606 val acc:83.00% \n",
            "Epoch 19/200 train loss:0.5301 train acc:74.14% \n",
            "Epoch 19/200 val loss:0.5904 val acc:71.00% \n",
            "Epoch 20/200 train loss:0.5568 train acc:67.98% \n",
            "Epoch 20/200 val loss:0.4613 val acc:77.00% \n",
            "Epoch 21/200 train loss:0.5063 train acc:73.29% \n",
            "Epoch 21/200 val loss:0.4319 val acc:84.00% \n",
            "Epoch 22/200 train loss:0.4951 train acc:74.66% \n",
            "Epoch 22/200 val loss:0.4204 val acc:83.00% \n",
            "Epoch 23/200 train loss:0.5233 train acc:72.43% \n",
            "Epoch 23/200 val loss:0.4530 val acc:82.00% \n",
            "Epoch 24/200 train loss:0.5259 train acc:72.09% \n",
            "Epoch 24/200 val loss:0.4015 val acc:85.00% \n",
            "Epoch 25/200 train loss:0.4880 train acc:75.51% \n",
            "Epoch 25/200 val loss:0.4143 val acc:81.00% \n",
            "Epoch 26/200 train loss:0.5055 train acc:74.66% \n",
            "Epoch 26/200 val loss:0.4188 val acc:82.00% \n",
            "Epoch 27/200 train loss:0.5026 train acc:73.29% \n",
            "Epoch 27/200 val loss:0.4743 val acc:78.00% \n",
            "Epoch 28/200 train loss:0.4919 train acc:73.80% \n",
            "Epoch 28/200 val loss:0.4223 val acc:81.00% \n",
            "Epoch 29/200 train loss:0.4849 train acc:75.68% \n",
            "Epoch 29/200 val loss:0.4181 val acc:81.00% \n",
            "Epoch 30/200 train loss:0.4740 train acc:77.05% \n",
            "Epoch 30/200 val loss:0.3862 val acc:84.00% \n",
            "Epoch 31/200 train loss:0.4718 train acc:78.42% \n",
            "Epoch 31/200 val loss:0.5214 val acc:78.00% \n",
            "Epoch 32/200 train loss:0.4738 train acc:76.03% \n",
            "Epoch 32/200 val loss:0.4062 val acc:83.00% \n",
            "Epoch 33/200 train loss:0.5294 train acc:69.86% \n",
            "Epoch 33/200 val loss:0.5364 val acc:77.00% \n",
            "Epoch 34/200 train loss:0.5266 train acc:67.47% \n",
            "Epoch 34/200 val loss:0.4965 val acc:82.00% \n",
            "Epoch 35/200 train loss:0.5080 train acc:75.00% \n",
            "Epoch 35/200 val loss:0.4071 val acc:83.00% \n",
            "Epoch 36/200 train loss:0.4882 train acc:73.97% \n",
            "Epoch 36/200 val loss:0.4265 val acc:82.00% \n",
            "Epoch 37/200 train loss:0.4802 train acc:76.37% \n",
            "Epoch 37/200 val loss:0.3897 val acc:83.00% \n",
            "Epoch 38/200 train loss:0.4808 train acc:76.54% \n",
            "Epoch 38/200 val loss:0.4083 val acc:85.00% \n",
            "Epoch 39/200 train loss:0.4715 train acc:77.91% \n",
            "Epoch 39/200 val loss:0.4095 val acc:83.00% \n",
            "Epoch 40/200 train loss:0.4924 train acc:75.86% \n",
            "Epoch 40/200 val loss:0.4475 val acc:84.00% \n",
            "Epoch 41/200 train loss:0.4766 train acc:77.74% \n",
            "Epoch 41/200 val loss:0.4118 val acc:84.00% \n",
            "Epoch 42/200 train loss:0.4658 train acc:76.20% \n",
            "Epoch 42/200 val loss:0.3890 val acc:84.00% \n",
            "Epoch 43/200 train loss:0.4723 train acc:77.05% \n",
            "Epoch 43/200 val loss:0.4217 val acc:83.00% \n",
            "Epoch 44/200 train loss:0.4911 train acc:75.51% \n",
            "Epoch 44/200 val loss:0.4037 val acc:82.00% \n",
            "Epoch 45/200 train loss:0.5193 train acc:75.17% \n",
            "Epoch 45/200 val loss:0.4416 val acc:80.00% \n",
            "Epoch 46/200 train loss:0.4663 train acc:77.23% \n",
            "Epoch 46/200 val loss:0.3994 val acc:84.00% \n",
            "Epoch 47/200 train loss:0.4781 train acc:77.05% \n",
            "Epoch 47/200 val loss:0.4708 val acc:79.00% \n",
            "Epoch 48/200 train loss:0.4753 train acc:77.40% \n",
            "Epoch 48/200 val loss:0.3636 val acc:85.00% \n",
            "Epoch 49/200 train loss:0.4653 train acc:78.08% \n",
            "Epoch 49/200 val loss:0.4754 val acc:79.00% \n",
            "Epoch 50/200 train loss:0.4867 train acc:73.97% \n",
            "Epoch 50/200 val loss:0.3872 val acc:83.00% \n",
            "Epoch 51/200 train loss:0.4595 train acc:75.51% \n",
            "Epoch 51/200 val loss:0.4055 val acc:84.00% \n",
            "Epoch 52/200 train loss:0.5846 train acc:76.03% \n",
            "Epoch 52/200 val loss:0.3820 val acc:85.00% \n",
            "Epoch 53/200 train loss:0.4816 train acc:75.17% \n",
            "Epoch 53/200 val loss:0.3818 val acc:84.00% \n",
            "Epoch 54/200 train loss:0.4825 train acc:76.20% \n",
            "Epoch 54/200 val loss:0.3934 val acc:83.00% \n",
            "Epoch 55/200 train loss:0.4670 train acc:75.86% \n",
            "Epoch 55/200 val loss:0.3889 val acc:84.00% \n",
            "Epoch 56/200 train loss:0.4632 train acc:77.40% \n",
            "Epoch 56/200 val loss:0.3679 val acc:83.00% \n",
            "Epoch 57/200 train loss:0.4521 train acc:77.57% \n",
            "Epoch 57/200 val loss:0.3982 val acc:84.00% \n",
            "Epoch 58/200 train loss:0.4751 train acc:75.34% \n",
            "Epoch 58/200 val loss:0.4301 val acc:82.00% \n",
            "Epoch 59/200 train loss:0.4645 train acc:76.88% \n",
            "Epoch 59/200 val loss:0.4182 val acc:83.00% \n",
            "Epoch 60/200 train loss:0.4560 train acc:77.57% \n",
            "Epoch 60/200 val loss:0.4400 val acc:79.00% \n",
            "Epoch 61/200 train loss:0.4636 train acc:76.71% \n",
            "Epoch 61/200 val loss:0.4153 val acc:79.00% \n",
            "Epoch 62/200 train loss:0.4521 train acc:76.54% \n",
            "Epoch 62/200 val loss:0.3782 val acc:84.00% \n",
            "Epoch 63/200 train loss:0.4486 train acc:75.00% \n",
            "Epoch 63/200 val loss:0.4264 val acc:82.00% \n",
            "Epoch 64/200 train loss:0.4900 train acc:75.00% \n",
            "Epoch 64/200 val loss:0.4095 val acc:84.00% \n",
            "Epoch 65/200 train loss:0.4560 train acc:75.00% \n",
            "Epoch 65/200 val loss:0.4033 val acc:81.00% \n",
            "Epoch 66/200 train loss:0.4450 train acc:76.88% \n",
            "Epoch 66/200 val loss:0.5046 val acc:69.00% \n",
            "Epoch 67/200 train loss:0.4656 train acc:73.80% \n",
            "Epoch 67/200 val loss:0.4271 val acc:74.00% \n",
            "Epoch 68/200 train loss:0.4700 train acc:77.57% \n",
            "Epoch 68/200 val loss:0.4241 val acc:82.00% \n",
            "Epoch 69/200 train loss:0.4558 train acc:74.66% \n",
            "Epoch 69/200 val loss:0.4260 val acc:81.00% \n",
            "Epoch 70/200 train loss:0.4387 train acc:77.05% \n",
            "Epoch 70/200 val loss:0.3688 val acc:83.00% \n",
            "Epoch 71/200 train loss:0.4412 train acc:76.88% \n",
            "Epoch 71/200 val loss:0.3941 val acc:83.00% \n",
            "Epoch 72/200 train loss:0.4494 train acc:77.91% \n",
            "Epoch 72/200 val loss:0.3820 val acc:84.00% \n",
            "Epoch 73/200 train loss:0.4583 train acc:76.71% \n",
            "Epoch 73/200 val loss:0.4171 val acc:84.00% \n",
            "Epoch 74/200 train loss:0.4353 train acc:77.23% \n",
            "Epoch 74/200 val loss:0.4072 val acc:82.00% \n",
            "Epoch 75/200 train loss:0.4434 train acc:74.83% \n",
            "Epoch 75/200 val loss:0.3791 val acc:85.00% \n",
            "Epoch 76/200 train loss:0.4263 train acc:79.28% \n",
            "Epoch 76/200 val loss:0.3854 val acc:85.00% \n",
            "Epoch 77/200 train loss:0.4504 train acc:78.60% \n",
            "Epoch 77/200 val loss:0.3753 val acc:83.00% \n",
            "Epoch 78/200 train loss:0.4385 train acc:78.08% \n",
            "Epoch 78/200 val loss:0.3766 val acc:81.00% \n",
            "Epoch 79/200 train loss:0.4366 train acc:77.57% \n",
            "Epoch 79/200 val loss:0.4121 val acc:82.00% \n",
            "Epoch 80/200 train loss:0.4283 train acc:75.68% \n",
            "Epoch 80/200 val loss:0.4372 val acc:76.00% \n",
            "Epoch 81/200 train loss:0.4316 train acc:76.88% \n",
            "Epoch 81/200 val loss:0.4022 val acc:76.00% \n",
            "Epoch 82/200 train loss:0.4384 train acc:79.11% \n",
            "Epoch 82/200 val loss:0.3794 val acc:84.00% \n",
            "Epoch 83/200 train loss:0.4423 train acc:78.08% \n",
            "Epoch 83/200 val loss:0.4440 val acc:75.00% \n",
            "Epoch 84/200 train loss:0.4415 train acc:76.54% \n",
            "Epoch 84/200 val loss:0.4100 val acc:78.00% \n",
            "Epoch 85/200 train loss:0.4042 train acc:79.79% \n",
            "Epoch 85/200 val loss:0.3768 val acc:78.00% \n",
            "Epoch 86/200 train loss:0.4262 train acc:76.88% \n",
            "Epoch 86/200 val loss:0.4091 val acc:85.00% \n",
            "Epoch 87/200 train loss:0.4079 train acc:78.08% \n",
            "Epoch 87/200 val loss:0.3722 val acc:81.00% \n",
            "Epoch 88/200 train loss:0.4442 train acc:77.05% \n",
            "Epoch 88/200 val loss:0.3669 val acc:84.00% \n",
            "Epoch 89/200 train loss:0.4184 train acc:77.23% \n",
            "Epoch 89/200 val loss:0.4212 val acc:74.00% \n",
            "Epoch 90/200 train loss:0.4138 train acc:79.45% \n",
            "Epoch 90/200 val loss:0.3825 val acc:83.00% \n",
            "Epoch 91/200 train loss:0.4570 train acc:77.23% \n",
            "Epoch 91/200 val loss:0.4042 val acc:80.00% \n",
            "Epoch 92/200 train loss:0.4189 train acc:77.05% \n",
            "Epoch 92/200 val loss:0.4384 val acc:74.00% \n",
            "Epoch 93/200 train loss:0.4368 train acc:78.94% \n",
            "Epoch 93/200 val loss:0.3942 val acc:83.00% \n",
            "Epoch 94/200 train loss:0.4326 train acc:77.40% \n",
            "Epoch 94/200 val loss:0.4073 val acc:76.00% \n",
            "Epoch 95/200 train loss:0.4243 train acc:77.40% \n",
            "Epoch 95/200 val loss:0.4085 val acc:82.00% \n",
            "Epoch 96/200 train loss:0.4212 train acc:74.83% \n",
            "Epoch 96/200 val loss:0.3921 val acc:84.00% \n",
            "Epoch 97/200 train loss:0.4268 train acc:79.11% \n",
            "Epoch 97/200 val loss:0.4070 val acc:77.00% \n",
            "Epoch 98/200 train loss:0.4203 train acc:78.08% \n",
            "Epoch 98/200 val loss:0.3987 val acc:83.00% \n",
            "Epoch 99/200 train loss:0.4100 train acc:78.94% \n",
            "Epoch 99/200 val loss:0.3871 val acc:84.00% \n",
            "Epoch 100/200 train loss:0.4115 train acc:77.74% \n",
            "Epoch 100/200 val loss:0.3815 val acc:80.00% \n",
            "Epoch 101/200 train loss:0.4066 train acc:78.42% \n",
            "Epoch 101/200 val loss:0.3753 val acc:78.00% \n",
            "Epoch 102/200 train loss:0.4200 train acc:78.77% \n",
            "Epoch 102/200 val loss:0.4096 val acc:77.00% \n",
            "Epoch 103/200 train loss:0.4004 train acc:79.28% \n",
            "Epoch 103/200 val loss:0.4291 val acc:84.00% \n",
            "Epoch 104/200 train loss:0.4067 train acc:79.62% \n",
            "Epoch 104/200 val loss:0.4248 val acc:78.00% \n",
            "Epoch 105/200 train loss:1.0200 train acc:64.21% \n",
            "Epoch 105/200 val loss:0.6086 val acc:67.00% \n",
            "Epoch 106/200 train loss:0.4993 train acc:76.54% \n",
            "Epoch 106/200 val loss:0.4131 val acc:74.00% \n",
            "Epoch 107/200 train loss:0.4678 train acc:79.28% \n",
            "Epoch 107/200 val loss:0.4410 val acc:79.00% \n",
            "Epoch 108/200 train loss:0.4297 train acc:79.28% \n",
            "Epoch 108/200 val loss:0.3936 val acc:82.00% \n",
            "Epoch 109/200 train loss:0.4104 train acc:78.42% \n",
            "Epoch 109/200 val loss:0.4124 val acc:79.00% \n",
            "Epoch 110/200 train loss:0.4132 train acc:78.42% \n",
            "Epoch 110/200 val loss:0.4037 val acc:76.00% \n",
            "Epoch 111/200 train loss:0.4039 train acc:76.71% \n",
            "Epoch 111/200 val loss:0.4967 val acc:70.00% \n",
            "Epoch 112/200 train loss:0.4036 train acc:79.45% \n",
            "Epoch 112/200 val loss:0.4118 val acc:81.00% \n",
            "Epoch 113/200 train loss:0.3983 train acc:79.11% \n",
            "Epoch 113/200 val loss:0.4080 val acc:82.00% \n",
            "Epoch 114/200 train loss:0.4154 train acc:79.28% \n",
            "Epoch 114/200 val loss:0.4196 val acc:75.00% \n",
            "Epoch 115/200 train loss:0.4024 train acc:77.05% \n",
            "Epoch 115/200 val loss:0.4461 val acc:81.00% \n",
            "Epoch 116/200 train loss:0.4026 train acc:79.62% \n",
            "Epoch 116/200 val loss:0.4574 val acc:80.00% \n",
            "Epoch 117/200 train loss:0.4223 train acc:77.23% \n",
            "Epoch 117/200 val loss:0.4845 val acc:73.00% \n",
            "Epoch 118/200 train loss:0.4322 train acc:78.77% \n",
            "Epoch 118/200 val loss:0.4149 val acc:74.00% \n",
            "Epoch 119/200 train loss:0.3971 train acc:76.88% \n",
            "Epoch 119/200 val loss:0.3857 val acc:80.00% \n",
            "Epoch 120/200 train loss:0.3942 train acc:79.45% \n",
            "Epoch 120/200 val loss:0.4086 val acc:77.00% \n",
            "Epoch 121/200 train loss:0.4094 train acc:78.42% \n",
            "Epoch 121/200 val loss:0.4114 val acc:76.00% \n",
            "Epoch 122/200 train loss:0.4041 train acc:80.99% \n",
            "Epoch 122/200 val loss:0.3737 val acc:84.00% \n",
            "Epoch 123/200 train loss:0.3972 train acc:79.79% \n",
            "Epoch 123/200 val loss:0.4684 val acc:75.00% \n",
            "Epoch 124/200 train loss:0.4012 train acc:78.08% \n",
            "Epoch 124/200 val loss:0.3905 val acc:84.00% \n",
            "Epoch 125/200 train loss:0.3937 train acc:79.79% \n",
            "Epoch 125/200 val loss:0.3964 val acc:77.00% \n",
            "Epoch 126/200 train loss:0.3844 train acc:80.82% \n",
            "Epoch 126/200 val loss:0.4238 val acc:82.00% \n",
            "Epoch 127/200 train loss:0.3982 train acc:79.11% \n",
            "Epoch 127/200 val loss:0.4118 val acc:81.00% \n",
            "Epoch 128/200 train loss:0.3961 train acc:78.94% \n",
            "Epoch 128/200 val loss:0.4426 val acc:77.00% \n",
            "Epoch 129/200 train loss:0.3777 train acc:80.31% \n",
            "Epoch 129/200 val loss:0.4080 val acc:83.00% \n",
            "Epoch 130/200 train loss:0.3959 train acc:78.77% \n",
            "Epoch 130/200 val loss:0.4372 val acc:76.00% \n",
            "Epoch 131/200 train loss:0.4068 train acc:77.40% \n",
            "Epoch 131/200 val loss:0.5373 val acc:80.00% \n",
            "Epoch 132/200 train loss:0.3914 train acc:78.42% \n",
            "Epoch 132/200 val loss:0.4320 val acc:76.00% \n",
            "Epoch 133/200 train loss:0.4272 train acc:79.62% \n",
            "Epoch 133/200 val loss:0.4212 val acc:82.00% \n",
            "Epoch 134/200 train loss:0.3872 train acc:79.62% \n",
            "Epoch 134/200 val loss:0.4069 val acc:79.00% \n",
            "Epoch 135/200 train loss:0.3842 train acc:80.14% \n",
            "Epoch 135/200 val loss:0.4498 val acc:75.00% \n",
            "Epoch 136/200 train loss:0.3776 train acc:79.28% \n",
            "Epoch 136/200 val loss:0.4412 val acc:75.00% \n",
            "Epoch 137/200 train loss:0.3673 train acc:79.62% \n",
            "Epoch 137/200 val loss:0.4530 val acc:79.00% \n",
            "Epoch 138/200 train loss:0.3916 train acc:80.65% \n",
            "Epoch 138/200 val loss:0.4307 val acc:73.00% \n",
            "Epoch 139/200 train loss:0.4117 train acc:78.08% \n",
            "Epoch 139/200 val loss:0.4063 val acc:81.00% \n",
            "Epoch 140/200 train loss:0.3847 train acc:78.77% \n",
            "Epoch 140/200 val loss:0.4344 val acc:77.00% \n",
            "Epoch 141/200 train loss:0.3903 train acc:79.62% \n",
            "Epoch 141/200 val loss:0.4077 val acc:80.00% \n",
            "Epoch 142/200 train loss:0.3777 train acc:80.82% \n",
            "Epoch 142/200 val loss:0.4321 val acc:79.00% \n",
            "Epoch 143/200 train loss:0.3832 train acc:79.97% \n",
            "Epoch 143/200 val loss:0.4237 val acc:80.00% \n",
            "Epoch 144/200 train loss:0.3782 train acc:81.68% \n",
            "Epoch 144/200 val loss:0.4225 val acc:78.00% \n",
            "Epoch 145/200 train loss:0.3872 train acc:80.82% \n",
            "Epoch 145/200 val loss:0.4553 val acc:77.00% \n",
            "Epoch 146/200 train loss:0.4592 train acc:77.74% \n",
            "Epoch 146/200 val loss:0.4181 val acc:76.00% \n",
            "Epoch 147/200 train loss:0.4444 train acc:78.94% \n",
            "Epoch 147/200 val loss:0.4092 val acc:83.00% \n",
            "Epoch 148/200 train loss:0.4062 train acc:77.57% \n",
            "Epoch 148/200 val loss:0.5230 val acc:83.00% \n",
            "Epoch 149/200 train loss:0.4945 train acc:77.40% \n",
            "Epoch 149/200 val loss:0.4714 val acc:80.00% \n",
            "Epoch 150/200 train loss:0.3705 train acc:79.79% \n",
            "Epoch 150/200 val loss:0.4558 val acc:80.00% \n",
            "Epoch 151/200 train loss:0.3795 train acc:79.28% \n",
            "Epoch 151/200 val loss:0.4454 val acc:80.00% \n",
            "Epoch 152/200 train loss:0.3904 train acc:80.48% \n",
            "Epoch 152/200 val loss:0.4220 val acc:82.00% \n",
            "Epoch 153/200 train loss:0.3765 train acc:79.45% \n",
            "Epoch 153/200 val loss:0.4235 val acc:81.00% \n",
            "Epoch 154/200 train loss:0.3857 train acc:80.99% \n",
            "Epoch 154/200 val loss:0.4682 val acc:80.00% \n",
            "Epoch 155/200 train loss:0.3827 train acc:78.60% \n",
            "Epoch 155/200 val loss:0.4398 val acc:76.00% \n",
            "Epoch 156/200 train loss:0.3732 train acc:81.16% \n",
            "Epoch 156/200 val loss:0.4452 val acc:81.00% \n",
            "Epoch 157/200 train loss:0.3726 train acc:80.99% \n",
            "Epoch 157/200 val loss:0.5526 val acc:78.00% \n",
            "Epoch 158/200 train loss:0.4066 train acc:78.94% \n",
            "Epoch 158/200 val loss:0.4201 val acc:81.00% \n",
            "Epoch 159/200 train loss:0.3689 train acc:80.99% \n",
            "Epoch 159/200 val loss:0.4502 val acc:80.00% \n",
            "Epoch 160/200 train loss:0.3662 train acc:81.34% \n",
            "Epoch 160/200 val loss:0.4579 val acc:81.00% \n",
            "Epoch 161/200 train loss:0.3940 train acc:78.77% \n",
            "Epoch 161/200 val loss:0.4217 val acc:81.00% \n",
            "Epoch 162/200 train loss:0.3683 train acc:80.48% \n",
            "Epoch 162/200 val loss:0.5645 val acc:76.00% \n",
            "Epoch 163/200 train loss:0.3702 train acc:79.79% \n",
            "Epoch 163/200 val loss:0.4542 val acc:79.00% \n",
            "Epoch 164/200 train loss:0.3923 train acc:81.85% \n",
            "Epoch 164/200 val loss:0.4696 val acc:79.00% \n",
            "Epoch 165/200 train loss:0.3843 train acc:80.99% \n",
            "Epoch 165/200 val loss:0.4289 val acc:77.00% \n",
            "Epoch 166/200 train loss:0.3731 train acc:81.85% \n",
            "Epoch 166/200 val loss:0.4731 val acc:74.00% \n",
            "Epoch 167/200 train loss:0.3814 train acc:81.16% \n",
            "Epoch 167/200 val loss:0.4946 val acc:74.00% \n",
            "Epoch 168/200 train loss:0.3683 train acc:80.14% \n",
            "Epoch 168/200 val loss:0.5083 val acc:76.00% \n",
            "Epoch 169/200 train loss:0.3490 train acc:80.31% \n",
            "Epoch 169/200 val loss:0.4773 val acc:78.00% \n",
            "Epoch 170/200 train loss:0.3764 train acc:81.85% \n",
            "Epoch 170/200 val loss:0.4473 val acc:80.00% \n",
            "Epoch 171/200 train loss:0.3543 train acc:83.22% \n",
            "Epoch 171/200 val loss:0.4676 val acc:79.00% \n",
            "Epoch 172/200 train loss:0.3566 train acc:81.16% \n",
            "Epoch 172/200 val loss:0.4939 val acc:79.00% \n",
            "Epoch 173/200 train loss:0.3927 train acc:80.65% \n",
            "Epoch 173/200 val loss:0.5303 val acc:75.00% \n",
            "Epoch 174/200 train loss:0.3751 train acc:81.34% \n",
            "Epoch 174/200 val loss:0.4447 val acc:78.00% \n",
            "Epoch 175/200 train loss:0.4228 train acc:80.14% \n",
            "Epoch 175/200 val loss:0.4698 val acc:79.00% \n",
            "Epoch 176/200 train loss:0.3568 train acc:82.19% \n",
            "Epoch 176/200 val loss:0.4884 val acc:76.00% \n",
            "Epoch 177/200 train loss:0.3331 train acc:83.39% \n",
            "Epoch 177/200 val loss:0.4780 val acc:72.00% \n",
            "Epoch 178/200 train loss:0.3957 train acc:78.94% \n",
            "Epoch 178/200 val loss:0.4663 val acc:75.00% \n",
            "Epoch 179/200 train loss:0.3640 train acc:82.88% \n",
            "Epoch 179/200 val loss:0.5002 val acc:76.00% \n",
            "Epoch 180/200 train loss:0.4877 train acc:79.62% \n",
            "Epoch 180/200 val loss:0.5176 val acc:77.00% \n",
            "Epoch 181/200 train loss:0.3544 train acc:81.51% \n",
            "Epoch 181/200 val loss:0.4707 val acc:79.00% \n",
            "Epoch 182/200 train loss:0.4005 train acc:79.11% \n",
            "Epoch 182/200 val loss:0.4876 val acc:75.00% \n",
            "Epoch 183/200 train loss:0.3705 train acc:81.68% \n",
            "Epoch 183/200 val loss:0.4510 val acc:80.00% \n",
            "Epoch 184/200 train loss:0.3684 train acc:80.82% \n",
            "Epoch 184/200 val loss:0.4537 val acc:76.00% \n",
            "Epoch 185/200 train loss:0.3374 train acc:80.31% \n",
            "Epoch 185/200 val loss:0.4709 val acc:78.00% \n",
            "Epoch 186/200 train loss:0.3516 train acc:82.88% \n",
            "Epoch 186/200 val loss:0.4539 val acc:78.00% \n",
            "Epoch 187/200 train loss:0.3409 train acc:80.65% \n",
            "Epoch 187/200 val loss:0.4553 val acc:79.00% \n",
            "Epoch 188/200 train loss:0.3755 train acc:79.79% \n",
            "Epoch 188/200 val loss:0.4692 val acc:76.00% \n",
            "Epoch 189/200 train loss:0.3446 train acc:83.05% \n",
            "Epoch 189/200 val loss:0.4621 val acc:81.00% \n",
            "Epoch 190/200 train loss:0.3631 train acc:82.36% \n",
            "Epoch 190/200 val loss:0.4843 val acc:75.00% \n",
            "Epoch 191/200 train loss:0.5464 train acc:78.94% \n",
            "Epoch 191/200 val loss:0.7409 val acc:65.00% \n",
            "Epoch 192/200 train loss:0.5506 train acc:75.68% \n",
            "Epoch 192/200 val loss:0.5583 val acc:74.00% \n",
            "Epoch 193/200 train loss:0.4343 train acc:79.28% \n",
            "Epoch 193/200 val loss:0.4304 val acc:82.00% \n",
            "Epoch 194/200 train loss:0.4166 train acc:80.31% \n",
            "Epoch 194/200 val loss:0.4282 val acc:82.00% \n",
            "Epoch 195/200 train loss:0.4600 train acc:79.28% \n",
            "Epoch 195/200 val loss:0.4395 val acc:81.00% \n",
            "Epoch 196/200 train loss:0.4371 train acc:79.97% \n",
            "Epoch 196/200 val loss:0.4598 val acc:82.00% \n",
            "Epoch 197/200 train loss:0.4089 train acc:80.14% \n",
            "Epoch 197/200 val loss:0.4961 val acc:82.00% \n",
            "Epoch 198/200 train loss:0.3895 train acc:80.31% \n",
            "Epoch 198/200 val loss:0.5032 val acc:75.00% \n",
            "Epoch 199/200 train loss:0.4025 train acc:82.53% \n",
            "Epoch 199/200 val loss:0.4481 val acc:77.00% \n",
            "Epoch 200/200 train loss:0.3751 train acc:81.16% \n",
            "Epoch 200/200 val loss:0.5028 val acc:75.00% \n"
          ]
        }
      ],
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "model = CNN_LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train_lstm(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm.pth\"))\n",
        "  \n",
        "  if correct_val == best_acc and best_train < correct_train:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm.pth\"))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKrwUrq7uNL5"
      },
      "source": [
        "## CNN_LSTM_ATT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9J7Kv-WJI5N",
        "outputId": "582f4913-b83d-48f7-c9c7-4bc314677c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 train loss:1.0357 train acc:42.64% \n",
            "Epoch 1/200 val loss:0.9610 val acc:35.00% \n",
            "Epoch 2/200 train loss:1.0011 train acc:51.54% \n",
            "Epoch 2/200 val loss:1.0445 val acc:59.00% \n",
            "Epoch 3/200 train loss:0.9815 train acc:48.97% \n",
            "Epoch 3/200 val loss:1.0195 val acc:54.00% \n",
            "Epoch 4/200 train loss:0.9543 train acc:55.65% \n",
            "Epoch 4/200 val loss:0.9913 val acc:57.00% \n",
            "Epoch 5/200 train loss:0.9740 train acc:49.83% \n",
            "Epoch 5/200 val loss:0.9485 val acc:58.00% \n",
            "Epoch 6/200 train loss:0.8934 train acc:51.88% \n",
            "Epoch 6/200 val loss:0.7982 val acc:51.00% \n",
            "Epoch 7/200 train loss:0.7878 train acc:55.14% \n",
            "Epoch 7/200 val loss:0.6595 val acc:74.00% \n",
            "Epoch 8/200 train loss:0.7241 train acc:61.47% \n",
            "Epoch 8/200 val loss:0.6126 val acc:70.00% \n",
            "Epoch 9/200 train loss:0.5849 train acc:67.64% \n",
            "Epoch 9/200 val loss:0.6113 val acc:68.00% \n",
            "Epoch 10/200 train loss:0.6306 train acc:64.04% \n",
            "Epoch 10/200 val loss:0.5440 val acc:67.00% \n",
            "Epoch 11/200 train loss:0.5608 train acc:66.95% \n",
            "Epoch 11/200 val loss:0.5514 val acc:69.00% \n",
            "Epoch 12/200 train loss:0.6027 train acc:70.03% \n",
            "Epoch 12/200 val loss:0.5475 val acc:74.00% \n",
            "Epoch 13/200 train loss:0.5789 train acc:71.58% \n",
            "Epoch 13/200 val loss:1.0092 val acc:67.00% \n",
            "Epoch 14/200 train loss:0.6116 train acc:71.58% \n",
            "Epoch 14/200 val loss:0.5433 val acc:77.00% \n",
            "Epoch 15/200 train loss:0.5146 train acc:72.95% \n",
            "Epoch 15/200 val loss:0.5346 val acc:74.00% \n",
            "Epoch 16/200 train loss:0.5385 train acc:74.49% \n",
            "Epoch 16/200 val loss:0.6215 val acc:67.00% \n",
            "Epoch 17/200 train loss:0.5090 train acc:73.63% \n",
            "Epoch 17/200 val loss:0.4798 val acc:79.00% \n",
            "Epoch 18/200 train loss:0.5655 train acc:71.92% \n",
            "Epoch 18/200 val loss:0.5562 val acc:74.00% \n",
            "Epoch 19/200 train loss:0.5142 train acc:75.86% \n",
            "Epoch 19/200 val loss:0.4892 val acc:77.00% \n",
            "Epoch 20/200 train loss:0.4851 train acc:75.86% \n",
            "Epoch 20/200 val loss:0.4684 val acc:80.00% \n",
            "Epoch 21/200 train loss:0.4910 train acc:76.88% \n",
            "Epoch 21/200 val loss:0.5252 val acc:80.00% \n",
            "Epoch 22/200 train loss:0.6357 train acc:72.95% \n",
            "Epoch 22/200 val loss:0.6516 val acc:75.00% \n",
            "Epoch 23/200 train loss:0.5177 train acc:75.17% \n",
            "Epoch 23/200 val loss:0.5195 val acc:77.00% \n",
            "Epoch 24/200 train loss:0.4878 train acc:75.34% \n",
            "Epoch 24/200 val loss:0.5689 val acc:78.00% \n",
            "Epoch 25/200 train loss:0.5075 train acc:75.00% \n",
            "Epoch 25/200 val loss:0.4646 val acc:80.00% \n",
            "Epoch 26/200 train loss:0.4784 train acc:75.86% \n",
            "Epoch 26/200 val loss:0.5448 val acc:75.00% \n",
            "Epoch 27/200 train loss:0.5304 train acc:75.00% \n",
            "Epoch 27/200 val loss:0.5188 val acc:76.00% \n",
            "Epoch 28/200 train loss:0.6708 train acc:73.80% \n",
            "Epoch 28/200 val loss:0.5257 val acc:75.00% \n",
            "Epoch 29/200 train loss:0.4734 train acc:76.88% \n",
            "Epoch 29/200 val loss:0.5097 val acc:78.00% \n",
            "Epoch 30/200 train loss:0.5007 train acc:76.37% \n",
            "Epoch 30/200 val loss:0.5007 val acc:76.00% \n",
            "Epoch 31/200 train loss:0.5017 train acc:74.32% \n",
            "Epoch 31/200 val loss:0.5342 val acc:76.00% \n",
            "Epoch 32/200 train loss:0.5546 train acc:74.49% \n",
            "Epoch 32/200 val loss:0.5107 val acc:78.00% \n",
            "Epoch 33/200 train loss:0.4724 train acc:77.05% \n",
            "Epoch 33/200 val loss:0.4777 val acc:80.00% \n",
            "Epoch 34/200 train loss:0.4738 train acc:77.57% \n",
            "Epoch 34/200 val loss:0.4638 val acc:80.00% \n",
            "Epoch 35/200 train loss:0.4711 train acc:77.40% \n",
            "Epoch 35/200 val loss:0.4547 val acc:80.00% \n",
            "Epoch 36/200 train loss:0.4793 train acc:76.88% \n",
            "Epoch 36/200 val loss:0.4861 val acc:80.00% \n",
            "Epoch 37/200 train loss:0.4850 train acc:78.08% \n",
            "Epoch 37/200 val loss:0.4929 val acc:79.00% \n",
            "Epoch 38/200 train loss:0.4584 train acc:79.11% \n",
            "Epoch 38/200 val loss:0.5500 val acc:78.00% \n",
            "Epoch 39/200 train loss:0.4696 train acc:77.91% \n",
            "Epoch 39/200 val loss:0.4926 val acc:78.00% \n",
            "Epoch 40/200 train loss:0.4467 train acc:78.25% \n",
            "Epoch 40/200 val loss:0.4602 val acc:80.00% \n",
            "Epoch 41/200 train loss:0.4744 train acc:77.74% \n",
            "Epoch 41/200 val loss:0.4789 val acc:81.00% \n",
            "Epoch 42/200 train loss:0.4761 train acc:77.74% \n",
            "Epoch 42/200 val loss:0.5066 val acc:81.00% \n",
            "Epoch 43/200 train loss:0.4583 train acc:78.08% \n",
            "Epoch 43/200 val loss:0.5316 val acc:74.00% \n",
            "Epoch 44/200 train loss:0.4609 train acc:77.05% \n",
            "Epoch 44/200 val loss:0.5254 val acc:75.00% \n",
            "Epoch 45/200 train loss:0.4770 train acc:78.25% \n",
            "Epoch 45/200 val loss:0.6002 val acc:73.00% \n",
            "Epoch 46/200 train loss:0.6384 train acc:72.43% \n",
            "Epoch 46/200 val loss:0.4632 val acc:79.00% \n",
            "Epoch 47/200 train loss:0.4804 train acc:76.71% \n",
            "Epoch 47/200 val loss:0.5266 val acc:71.00% \n",
            "Epoch 48/200 train loss:0.4772 train acc:77.40% \n",
            "Epoch 48/200 val loss:0.4808 val acc:80.00% \n",
            "Epoch 49/200 train loss:0.4523 train acc:77.74% \n",
            "Epoch 49/200 val loss:0.4468 val acc:80.00% \n",
            "Epoch 50/200 train loss:0.4422 train acc:78.08% \n",
            "Epoch 50/200 val loss:0.4936 val acc:77.00% \n",
            "Epoch 51/200 train loss:0.4566 train acc:77.57% \n",
            "Epoch 51/200 val loss:0.4441 val acc:76.00% \n",
            "Epoch 52/200 train loss:0.4372 train acc:76.54% \n",
            "Epoch 52/200 val loss:0.4367 val acc:80.00% \n",
            "Epoch 53/200 train loss:0.4362 train acc:78.42% \n",
            "Epoch 53/200 val loss:0.3910 val acc:81.00% \n",
            "Epoch 54/200 train loss:0.4783 train acc:74.32% \n",
            "Epoch 54/200 val loss:0.5390 val acc:78.00% \n",
            "Epoch 55/200 train loss:0.4787 train acc:77.05% \n",
            "Epoch 55/200 val loss:0.4706 val acc:80.00% \n",
            "Epoch 56/200 train loss:0.4475 train acc:78.08% \n",
            "Epoch 56/200 val loss:0.4654 val acc:80.00% \n",
            "Epoch 57/200 train loss:0.4224 train acc:78.25% \n",
            "Epoch 57/200 val loss:0.5192 val acc:77.00% \n",
            "Epoch 58/200 train loss:0.4461 train acc:78.08% \n",
            "Epoch 58/200 val loss:0.4639 val acc:77.00% \n",
            "Epoch 59/200 train loss:0.4760 train acc:77.23% \n",
            "Epoch 59/200 val loss:0.4799 val acc:78.00% \n",
            "Epoch 60/200 train loss:0.4366 train acc:78.42% \n",
            "Epoch 60/200 val loss:0.4922 val acc:79.00% \n",
            "Epoch 61/200 train loss:0.4479 train acc:78.08% \n",
            "Epoch 61/200 val loss:0.4815 val acc:80.00% \n",
            "Epoch 62/200 train loss:0.4617 train acc:78.08% \n",
            "Epoch 62/200 val loss:0.4964 val acc:80.00% \n",
            "Epoch 63/200 train loss:0.4366 train acc:77.74% \n",
            "Epoch 63/200 val loss:0.4223 val acc:80.00% \n",
            "Epoch 64/200 train loss:0.4814 train acc:77.40% \n",
            "Epoch 64/200 val loss:0.4830 val acc:78.00% \n",
            "Epoch 65/200 train loss:0.4370 train acc:77.40% \n",
            "Epoch 65/200 val loss:0.5089 val acc:75.00% \n",
            "Epoch 66/200 train loss:0.4281 train acc:77.23% \n",
            "Epoch 66/200 val loss:0.4999 val acc:75.00% \n",
            "Epoch 67/200 train loss:0.4563 train acc:75.51% \n",
            "Epoch 67/200 val loss:0.4302 val acc:79.00% \n",
            "Epoch 68/200 train loss:0.4297 train acc:79.11% \n",
            "Epoch 68/200 val loss:0.4697 val acc:76.00% \n",
            "Epoch 69/200 train loss:0.4387 train acc:77.91% \n",
            "Epoch 69/200 val loss:0.4666 val acc:78.00% \n",
            "Epoch 70/200 train loss:0.4568 train acc:76.88% \n",
            "Epoch 70/200 val loss:0.4923 val acc:75.00% \n",
            "Epoch 71/200 train loss:0.4540 train acc:78.60% \n",
            "Epoch 71/200 val loss:0.4708 val acc:75.00% \n",
            "Epoch 72/200 train loss:0.4330 train acc:79.11% \n",
            "Epoch 72/200 val loss:0.5386 val acc:74.00% \n",
            "Epoch 73/200 train loss:0.4425 train acc:77.74% \n",
            "Epoch 73/200 val loss:0.4384 val acc:77.00% \n",
            "Epoch 74/200 train loss:0.4317 train acc:77.57% \n",
            "Epoch 74/200 val loss:0.5594 val acc:77.00% \n",
            "Epoch 75/200 train loss:0.4466 train acc:77.40% \n",
            "Epoch 75/200 val loss:0.4756 val acc:74.00% \n",
            "Epoch 76/200 train loss:0.4466 train acc:78.60% \n",
            "Epoch 76/200 val loss:0.4292 val acc:78.00% \n",
            "Epoch 77/200 train loss:0.4846 train acc:75.86% \n",
            "Epoch 77/200 val loss:0.4438 val acc:78.00% \n",
            "Epoch 78/200 train loss:0.4227 train acc:80.31% \n",
            "Epoch 78/200 val loss:0.4737 val acc:73.00% \n",
            "Epoch 79/200 train loss:0.4351 train acc:76.88% \n",
            "Epoch 79/200 val loss:0.5000 val acc:75.00% \n",
            "Epoch 80/200 train loss:0.4734 train acc:74.83% \n",
            "Epoch 80/200 val loss:0.4302 val acc:79.00% \n",
            "Epoch 81/200 train loss:0.4223 train acc:78.42% \n",
            "Epoch 81/200 val loss:0.4120 val acc:81.00% \n",
            "Epoch 82/200 train loss:0.4715 train acc:76.37% \n",
            "Epoch 82/200 val loss:0.5360 val acc:76.00% \n",
            "Epoch 83/200 train loss:0.4652 train acc:76.71% \n",
            "Epoch 83/200 val loss:0.5750 val acc:76.00% \n",
            "Epoch 84/200 train loss:0.4325 train acc:78.25% \n",
            "Epoch 84/200 val loss:0.4707 val acc:79.00% \n",
            "Epoch 85/200 train loss:0.4341 train acc:78.60% \n",
            "Epoch 85/200 val loss:0.5879 val acc:72.00% \n",
            "Epoch 86/200 train loss:0.4445 train acc:77.74% \n",
            "Epoch 86/200 val loss:0.5187 val acc:75.00% \n",
            "Epoch 87/200 train loss:0.4353 train acc:76.37% \n",
            "Epoch 87/200 val loss:0.4891 val acc:77.00% \n",
            "Epoch 88/200 train loss:0.4421 train acc:77.74% \n",
            "Epoch 88/200 val loss:0.4659 val acc:79.00% \n",
            "Epoch 89/200 train loss:0.4351 train acc:76.88% \n",
            "Epoch 89/200 val loss:0.4702 val acc:81.00% \n",
            "Epoch 90/200 train loss:0.4202 train acc:78.77% \n",
            "Epoch 90/200 val loss:0.4060 val acc:81.00% \n",
            "Epoch 91/200 train loss:0.4209 train acc:78.25% \n",
            "Epoch 91/200 val loss:0.4336 val acc:81.00% \n",
            "Epoch 92/200 train loss:0.4586 train acc:77.40% \n",
            "Epoch 92/200 val loss:0.5333 val acc:75.00% \n",
            "Epoch 93/200 train loss:0.4226 train acc:77.05% \n",
            "Epoch 93/200 val loss:0.4198 val acc:76.00% \n",
            "Epoch 94/200 train loss:0.4346 train acc:76.71% \n",
            "Epoch 94/200 val loss:0.4780 val acc:75.00% \n",
            "Epoch 95/200 train loss:0.4376 train acc:77.91% \n",
            "Epoch 95/200 val loss:0.4287 val acc:81.00% \n",
            "Epoch 96/200 train loss:0.4408 train acc:77.23% \n",
            "Epoch 96/200 val loss:0.4859 val acc:70.00% \n",
            "Epoch 97/200 train loss:0.4446 train acc:76.88% \n",
            "Epoch 97/200 val loss:0.4396 val acc:78.00% \n",
            "Epoch 98/200 train loss:0.4285 train acc:78.08% \n",
            "Epoch 98/200 val loss:0.4433 val acc:80.00% \n",
            "Epoch 99/200 train loss:0.4239 train acc:78.42% \n",
            "Epoch 99/200 val loss:0.4580 val acc:79.00% \n",
            "Epoch 100/200 train loss:0.4099 train acc:79.45% \n",
            "Epoch 100/200 val loss:0.4012 val acc:77.00% \n",
            "Epoch 101/200 train loss:0.4174 train acc:75.86% \n",
            "Epoch 101/200 val loss:0.4360 val acc:80.00% \n",
            "Epoch 102/200 train loss:0.4414 train acc:75.86% \n",
            "Epoch 102/200 val loss:0.3925 val acc:80.00% \n",
            "Epoch 103/200 train loss:0.4346 train acc:78.60% \n",
            "Epoch 103/200 val loss:0.4794 val acc:77.00% \n",
            "Epoch 104/200 train loss:0.5470 train acc:76.88% \n",
            "Epoch 104/200 val loss:0.4505 val acc:79.00% \n",
            "Epoch 105/200 train loss:0.4484 train acc:78.60% \n",
            "Epoch 105/200 val loss:0.4077 val acc:82.00% \n",
            "Epoch 106/200 train loss:0.4415 train acc:78.25% \n",
            "Epoch 106/200 val loss:0.4301 val acc:81.00% \n",
            "Epoch 107/200 train loss:0.4322 train acc:79.45% \n",
            "Epoch 107/200 val loss:0.4936 val acc:81.00% \n",
            "Epoch 108/200 train loss:0.4157 train acc:79.79% \n",
            "Epoch 108/200 val loss:0.4325 val acc:79.00% \n",
            "Epoch 109/200 train loss:0.4209 train acc:78.77% \n",
            "Epoch 109/200 val loss:0.4266 val acc:81.00% \n",
            "Epoch 110/200 train loss:0.4349 train acc:78.25% \n",
            "Epoch 110/200 val loss:0.4528 val acc:81.00% \n",
            "Epoch 111/200 train loss:0.4037 train acc:79.11% \n",
            "Epoch 111/200 val loss:0.4236 val acc:81.00% \n",
            "Epoch 112/200 train loss:0.4202 train acc:78.60% \n",
            "Epoch 112/200 val loss:0.4323 val acc:79.00% \n",
            "Epoch 113/200 train loss:0.4267 train acc:78.77% \n",
            "Epoch 113/200 val loss:0.4519 val acc:82.00% \n",
            "Epoch 114/200 train loss:0.4156 train acc:79.97% \n",
            "Epoch 114/200 val loss:0.4701 val acc:77.00% \n",
            "Epoch 115/200 train loss:0.4236 train acc:79.97% \n",
            "Epoch 115/200 val loss:0.4400 val acc:76.00% \n",
            "Epoch 116/200 train loss:0.4167 train acc:80.31% \n",
            "Epoch 116/200 val loss:0.4473 val acc:75.00% \n",
            "Epoch 117/200 train loss:0.4432 train acc:78.25% \n",
            "Epoch 117/200 val loss:0.4606 val acc:79.00% \n",
            "Epoch 118/200 train loss:0.4250 train acc:78.25% \n",
            "Epoch 118/200 val loss:0.4788 val acc:81.00% \n",
            "Epoch 119/200 train loss:0.4239 train acc:76.20% \n",
            "Epoch 119/200 val loss:0.4051 val acc:82.00% \n",
            "Epoch 120/200 train loss:0.4083 train acc:79.45% \n",
            "Epoch 120/200 val loss:0.4730 val acc:81.00% \n",
            "Epoch 121/200 train loss:0.4066 train acc:78.94% \n",
            "Epoch 121/200 val loss:0.4083 val acc:82.00% \n",
            "Epoch 122/200 train loss:0.4288 train acc:78.08% \n",
            "Epoch 122/200 val loss:0.4481 val acc:83.00% \n",
            "Epoch 123/200 train loss:0.4103 train acc:79.45% \n",
            "Epoch 123/200 val loss:0.4618 val acc:81.00% \n",
            "Epoch 124/200 train loss:0.4246 train acc:78.25% \n",
            "Epoch 124/200 val loss:0.4888 val acc:78.00% \n",
            "Epoch 125/200 train loss:0.4014 train acc:78.42% \n",
            "Epoch 125/200 val loss:0.4640 val acc:80.00% \n",
            "Epoch 126/200 train loss:0.4041 train acc:79.79% \n",
            "Epoch 126/200 val loss:0.4351 val acc:81.00% \n",
            "Epoch 127/200 train loss:0.4269 train acc:78.08% \n",
            "Epoch 127/200 val loss:0.5049 val acc:82.00% \n",
            "Epoch 128/200 train loss:0.4378 train acc:79.45% \n",
            "Epoch 128/200 val loss:0.3897 val acc:82.00% \n",
            "Epoch 129/200 train loss:0.4282 train acc:77.91% \n",
            "Epoch 129/200 val loss:0.5019 val acc:77.00% \n",
            "Epoch 130/200 train loss:0.4317 train acc:76.88% \n",
            "Epoch 130/200 val loss:0.4413 val acc:76.00% \n",
            "Epoch 131/200 train loss:0.4054 train acc:76.37% \n",
            "Epoch 131/200 val loss:0.4373 val acc:77.00% \n",
            "Epoch 132/200 train loss:0.3993 train acc:78.94% \n",
            "Epoch 132/200 val loss:0.4398 val acc:82.00% \n",
            "Epoch 133/200 train loss:0.4263 train acc:78.25% \n",
            "Epoch 133/200 val loss:0.4691 val acc:80.00% \n",
            "Epoch 134/200 train loss:0.4146 train acc:78.60% \n",
            "Epoch 134/200 val loss:0.4976 val acc:81.00% \n",
            "Epoch 135/200 train loss:0.4004 train acc:78.77% \n",
            "Epoch 135/200 val loss:0.4594 val acc:75.00% \n",
            "Epoch 136/200 train loss:0.4215 train acc:77.91% \n",
            "Epoch 136/200 val loss:0.5251 val acc:82.00% \n",
            "Epoch 137/200 train loss:0.4113 train acc:78.77% \n",
            "Epoch 137/200 val loss:0.5056 val acc:74.00% \n",
            "Epoch 138/200 train loss:0.4129 train acc:79.28% \n",
            "Epoch 138/200 val loss:0.4763 val acc:83.00% \n",
            "Epoch 139/200 train loss:0.4116 train acc:78.60% \n",
            "Epoch 139/200 val loss:0.4755 val acc:79.00% \n",
            "Epoch 140/200 train loss:0.4154 train acc:79.45% \n",
            "Epoch 140/200 val loss:0.5041 val acc:82.00% \n",
            "Epoch 141/200 train loss:0.4110 train acc:78.42% \n",
            "Epoch 141/200 val loss:0.4631 val acc:75.00% \n",
            "Epoch 142/200 train loss:0.4199 train acc:79.62% \n",
            "Epoch 142/200 val loss:0.4167 val acc:80.00% \n",
            "Epoch 143/200 train loss:0.4020 train acc:79.79% \n",
            "Epoch 143/200 val loss:0.3964 val acc:81.00% \n",
            "Epoch 144/200 train loss:0.4401 train acc:78.25% \n",
            "Epoch 144/200 val loss:0.4429 val acc:76.00% \n",
            "Epoch 145/200 train loss:0.4192 train acc:79.28% \n",
            "Epoch 145/200 val loss:0.4701 val acc:84.00% \n",
            "Epoch 146/200 train loss:0.4040 train acc:78.77% \n",
            "Epoch 146/200 val loss:0.4471 val acc:80.00% \n",
            "Epoch 147/200 train loss:0.4088 train acc:77.74% \n",
            "Epoch 147/200 val loss:0.4276 val acc:77.00% \n",
            "Epoch 148/200 train loss:0.4030 train acc:78.77% \n",
            "Epoch 148/200 val loss:0.4120 val acc:82.00% \n",
            "Epoch 149/200 train loss:0.3919 train acc:79.28% \n",
            "Epoch 149/200 val loss:0.3888 val acc:76.00% \n",
            "Epoch 150/200 train loss:0.4096 train acc:77.23% \n",
            "Epoch 150/200 val loss:0.5591 val acc:77.00% \n",
            "Epoch 151/200 train loss:0.4342 train acc:76.71% \n",
            "Epoch 151/200 val loss:0.4789 val acc:74.00% \n",
            "Epoch 152/200 train loss:0.4532 train acc:76.03% \n",
            "Epoch 152/200 val loss:0.4644 val acc:78.00% \n",
            "Epoch 153/200 train loss:0.4895 train acc:77.74% \n",
            "Epoch 153/200 val loss:0.4874 val acc:82.00% \n",
            "Epoch 154/200 train loss:0.4225 train acc:79.79% \n",
            "Epoch 154/200 val loss:0.5124 val acc:75.00% \n",
            "Epoch 155/200 train loss:0.4232 train acc:78.94% \n",
            "Epoch 155/200 val loss:0.4914 val acc:75.00% \n",
            "Epoch 156/200 train loss:0.3944 train acc:80.14% \n",
            "Epoch 156/200 val loss:0.5182 val acc:75.00% \n",
            "Epoch 157/200 train loss:0.4088 train acc:77.57% \n",
            "Epoch 157/200 val loss:0.4257 val acc:79.00% \n",
            "Epoch 158/200 train loss:0.3878 train acc:81.51% \n",
            "Epoch 158/200 val loss:0.4862 val acc:75.00% \n",
            "Epoch 159/200 train loss:0.4024 train acc:79.79% \n",
            "Epoch 159/200 val loss:0.4690 val acc:77.00% \n",
            "Epoch 160/200 train loss:0.4208 train acc:80.99% \n",
            "Epoch 160/200 val loss:0.4499 val acc:75.00% \n",
            "Epoch 161/200 train loss:0.4136 train acc:78.94% \n",
            "Epoch 161/200 val loss:0.5237 val acc:72.00% \n",
            "Epoch 162/200 train loss:0.4107 train acc:80.31% \n",
            "Epoch 162/200 val loss:0.4644 val acc:82.00% \n",
            "Epoch 163/200 train loss:0.4047 train acc:79.79% \n",
            "Epoch 163/200 val loss:0.4238 val acc:81.00% \n",
            "Epoch 164/200 train loss:0.4011 train acc:78.42% \n",
            "Epoch 164/200 val loss:0.4265 val acc:78.00% \n",
            "Epoch 165/200 train loss:0.4153 train acc:78.42% \n",
            "Epoch 165/200 val loss:0.4551 val acc:80.00% \n",
            "Epoch 166/200 train loss:0.3889 train acc:79.28% \n",
            "Epoch 166/200 val loss:0.4392 val acc:82.00% \n",
            "Epoch 167/200 train loss:0.3809 train acc:79.62% \n",
            "Epoch 167/200 val loss:0.5259 val acc:78.00% \n",
            "Epoch 168/200 train loss:0.3983 train acc:81.34% \n",
            "Epoch 168/200 val loss:0.5575 val acc:76.00% \n",
            "Epoch 169/200 train loss:0.4247 train acc:78.25% \n",
            "Epoch 169/200 val loss:0.4994 val acc:77.00% \n",
            "Epoch 170/200 train loss:0.3950 train acc:79.79% \n",
            "Epoch 170/200 val loss:0.4105 val acc:85.00% \n",
            "Epoch 171/200 train loss:0.4155 train acc:79.79% \n",
            "Epoch 171/200 val loss:0.4873 val acc:74.00% \n",
            "Epoch 172/200 train loss:0.3906 train acc:78.77% \n",
            "Epoch 172/200 val loss:0.4593 val acc:78.00% \n",
            "Epoch 173/200 train loss:0.3994 train acc:79.62% \n",
            "Epoch 173/200 val loss:0.4577 val acc:75.00% \n",
            "Epoch 174/200 train loss:0.3960 train acc:79.79% \n",
            "Epoch 174/200 val loss:0.4773 val acc:81.00% \n",
            "Epoch 175/200 train loss:0.4056 train acc:79.45% \n",
            "Epoch 175/200 val loss:0.5036 val acc:73.00% \n",
            "Epoch 176/200 train loss:0.3979 train acc:79.11% \n",
            "Epoch 176/200 val loss:0.4658 val acc:81.00% \n",
            "Epoch 177/200 train loss:0.3829 train acc:80.14% \n",
            "Epoch 177/200 val loss:0.4764 val acc:78.00% \n",
            "Epoch 178/200 train loss:0.3925 train acc:80.14% \n",
            "Epoch 178/200 val loss:0.3779 val acc:83.00% \n",
            "Epoch 179/200 train loss:0.4046 train acc:78.08% \n",
            "Epoch 179/200 val loss:0.4647 val acc:77.00% \n",
            "Epoch 180/200 train loss:0.3855 train acc:80.14% \n",
            "Epoch 180/200 val loss:0.4908 val acc:74.00% \n",
            "Epoch 181/200 train loss:0.4033 train acc:80.31% \n",
            "Epoch 181/200 val loss:0.5103 val acc:76.00% \n",
            "Epoch 182/200 train loss:0.3832 train acc:81.34% \n",
            "Epoch 182/200 val loss:0.3875 val acc:78.00% \n",
            "Epoch 183/200 train loss:0.4102 train acc:78.60% \n",
            "Epoch 183/200 val loss:0.3637 val acc:81.00% \n",
            "Epoch 184/200 train loss:0.4050 train acc:79.79% \n",
            "Epoch 184/200 val loss:0.4240 val acc:81.00% \n",
            "Epoch 185/200 train loss:0.4073 train acc:79.79% \n",
            "Epoch 185/200 val loss:0.4387 val acc:85.00% \n",
            "Epoch 186/200 train loss:0.3947 train acc:80.31% \n",
            "Epoch 186/200 val loss:0.4475 val acc:75.00% \n",
            "Epoch 187/200 train loss:0.3829 train acc:79.79% \n",
            "Epoch 187/200 val loss:0.4087 val acc:83.00% \n",
            "Epoch 188/200 train loss:0.4081 train acc:78.77% \n",
            "Epoch 188/200 val loss:0.4693 val acc:81.00% \n",
            "Epoch 189/200 train loss:0.3813 train acc:81.68% \n",
            "Epoch 189/200 val loss:0.4628 val acc:81.00% \n",
            "Epoch 190/200 train loss:0.3803 train acc:80.99% \n",
            "Epoch 190/200 val loss:0.3929 val acc:81.00% \n",
            "Epoch 191/200 train loss:0.3818 train acc:79.28% \n",
            "Epoch 191/200 val loss:0.3600 val acc:81.00% \n",
            "Epoch 192/200 train loss:0.3717 train acc:79.97% \n",
            "Epoch 192/200 val loss:0.3957 val acc:79.00% \n",
            "Epoch 193/200 train loss:0.3847 train acc:79.45% \n",
            "Epoch 193/200 val loss:0.3966 val acc:76.00% \n",
            "Epoch 194/200 train loss:0.3781 train acc:80.31% \n",
            "Epoch 194/200 val loss:0.4524 val acc:77.00% \n",
            "Epoch 195/200 train loss:0.3905 train acc:80.65% \n",
            "Epoch 195/200 val loss:0.5291 val acc:78.00% \n",
            "Epoch 196/200 train loss:0.3900 train acc:80.65% \n",
            "Epoch 196/200 val loss:0.4838 val acc:75.00% \n",
            "Epoch 197/200 train loss:0.3931 train acc:80.31% \n",
            "Epoch 197/200 val loss:0.5394 val acc:79.00% \n",
            "Epoch 198/200 train loss:0.3734 train acc:78.42% \n",
            "Epoch 198/200 val loss:0.4500 val acc:77.00% \n",
            "Epoch 199/200 train loss:0.3833 train acc:80.48% \n",
            "Epoch 199/200 val loss:0.4629 val acc:77.00% \n",
            "Epoch 200/200 train loss:0.3785 train acc:81.85% \n",
            "Epoch 200/200 val loss:0.4438 val acc:74.00% \n"
          ]
        }
      ],
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "model = CNN_LSTM_att().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train_lstm(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm_att.pth\"))\n",
        "  \n",
        "  if correct_val == best_acc and best_train < correct_train:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm_att.pth\"))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "task1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45c34a5a16294aed87ddbace2ebd3db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a1fc65a65184d57a33432d405e78f2a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbe515daaa0844a49b653394f480062b",
              "IPY_MODEL_0d39cd3e34fc431cb8ae06d272665a10",
              "IPY_MODEL_7af9bbbb5c394e0f804992cfee29372b"
            ]
          }
        },
        "2a1fc65a65184d57a33432d405e78f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbe515daaa0844a49b653394f480062b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8bc4ba9ea5eb4aee996c6211acd23ebb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be4eed98792443f0b919c7a9b505b3fe"
          }
        },
        "0d39cd3e34fc431cb8ae06d272665a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b111da5122a46a598240bfbc9b891f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53142ff8983a4d838db5ad55cfe83ab4"
          }
        },
        "7af9bbbb5c394e0f804992cfee29372b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_584a82a6faa84ad59e00673954b80240",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [33:33&lt;00:00,  1.31s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec0dd012874849639c7e3a170e2ae3bf"
          }
        },
        "8bc4ba9ea5eb4aee996c6211acd23ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be4eed98792443f0b919c7a9b505b3fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b111da5122a46a598240bfbc9b891f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53142ff8983a4d838db5ad55cfe83ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "584a82a6faa84ad59e00673954b80240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec0dd012874849639c7e3a170e2ae3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "292ddece32df448b8a2d57427de8f5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e1d81abef9c44d3932022aa10cf0ac9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_38c016ad73364d8fab0cc78a7799a8f8",
              "IPY_MODEL_3aa4bd646fb24546b24260d5a1ef7fcc",
              "IPY_MODEL_a2158cb8927c4dad91713adef50624ab"
            ]
          }
        },
        "6e1d81abef9c44d3932022aa10cf0ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38c016ad73364d8fab0cc78a7799a8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7793fc99332f494595ba26a43003fb65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6ff583cc9894b8596636e9b344389a0"
          }
        },
        "3aa4bd646fb24546b24260d5a1ef7fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e51791b70984af5b62e0ab30989064d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e700068b2aa44ac7836f8d5623931d5b"
          }
        },
        "a2158cb8927c4dad91713adef50624ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de1d4496ad44424ab00b32aa855d86ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [31:13&lt;00:00,  1.15s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58bd3edb50d4434c8d75e7d8a16d54bb"
          }
        },
        "7793fc99332f494595ba26a43003fb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6ff583cc9894b8596636e9b344389a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e51791b70984af5b62e0ab30989064d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e700068b2aa44ac7836f8d5623931d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de1d4496ad44424ab00b32aa855d86ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58bd3edb50d4434c8d75e7d8a16d54bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}